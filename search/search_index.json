{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to melusine Overview Melusine is a high-level library for emails processing that can be used to : Categorize emails using AI, regex patterns or both Prioritize urgent emails Extract information And much more ! Why melusine ? The added value of melusine mainly resides in the following aspects: Off-the-shelf features : melusine comes with a number of features that can be used straightaway Segmenting messages in an email conversation Tagging message parts (Email body, signatures, footers, etc) Transferred email handling Execution framework : users can focus on the email qualification code and save time on the boilerplate code debug mode pipeline execution code parallelization etc Integrations : the modular nature of melusine makes it easy to integrate with a variety of AI frameworks (HuggingFace, Pytorch, Tensorflow, etc) Production ready : melusine builds-up on the feedback from several years of running automatic email processing in production at MAIF. The melusine package melusine/ docs/ # Documentation (using mkdocs-material). exemples/ # Tutorials and exemples src/ # Sources of the melusine package. backend/ # Define execution backends (JSON, Pandas, Polars, etc) conf/ # Configuration loading and default conf data/ # Dummy data for examples and prototyping io/ # Save/Load operations models/ # AI/ML related features regex/ # Regex related code testing/ # Pipeline testing code tests/ # Extensive testing of the code and the tutorials. Getting started Get started with melusine following our (tested!) tutorials: Getting Started MelusinePipeline MelusineTransformers MelusineRegex ML models MelusineDetector Configurations Basic Classification","title":"Welcome to melusine"},{"location":"#welcome-to-melusine","text":"","title":"Welcome to melusine"},{"location":"#overview","text":"Melusine is a high-level library for emails processing that can be used to : Categorize emails using AI, regex patterns or both Prioritize urgent emails Extract information And much more !","title":"Overview"},{"location":"#why-melusine","text":"The added value of melusine mainly resides in the following aspects: Off-the-shelf features : melusine comes with a number of features that can be used straightaway Segmenting messages in an email conversation Tagging message parts (Email body, signatures, footers, etc) Transferred email handling Execution framework : users can focus on the email qualification code and save time on the boilerplate code debug mode pipeline execution code parallelization etc Integrations : the modular nature of melusine makes it easy to integrate with a variety of AI frameworks (HuggingFace, Pytorch, Tensorflow, etc) Production ready : melusine builds-up on the feedback from several years of running automatic email processing in production at MAIF.","title":"Why melusine ?"},{"location":"#the-melusine-package","text":"melusine/ docs/ # Documentation (using mkdocs-material). exemples/ # Tutorials and exemples src/ # Sources of the melusine package. backend/ # Define execution backends (JSON, Pandas, Polars, etc) conf/ # Configuration loading and default conf data/ # Dummy data for examples and prototyping io/ # Save/Load operations models/ # AI/ML related features regex/ # Regex related code testing/ # Pipeline testing code tests/ # Extensive testing of the code and the tutorials.","title":"The melusine package"},{"location":"#getting-started","text":"Get started with melusine following our (tested!) tutorials: Getting Started MelusinePipeline MelusineTransformers MelusineRegex ML models MelusineDetector Configurations Basic Classification","title":"Getting started"},{"location":"advanced/ContentTagger/","text":"Use custom message tags","title":"Use custom message tags"},{"location":"advanced/ContentTagger/#use-custom-message-tags","text":"","title":"Use custom message tags"},{"location":"advanced/CustomDetector/","text":"Use a custom MelusineDetector template Specify abstract methods Row transformations vs dataframe transformations","title":"Use a custom MelusineDetector template"},{"location":"advanced/CustomDetector/#use-a-custom-melusinedetector-template","text":"","title":"Use a custom MelusineDetector template"},{"location":"advanced/CustomDetector/#specify-abstract-methods","text":"","title":"Specify abstract methods"},{"location":"advanced/CustomDetector/#row-transformations-vs-dataframe-transformations","text":"","title":"Row transformations vs dataframe transformations"},{"location":"advanced/ExchangeConnector/","text":"Connect melusine to a Microsoft Exchange Mailbox","title":"Connect melusine to a Microsoft Exchange Mailbox"},{"location":"advanced/ExchangeConnector/#connect-melusine-to-a-microsoft-exchange-mailbox","text":"","title":"Connect melusine to a Microsoft Exchange Mailbox"},{"location":"advanced/PreTrainedModelsHF/","text":"Use pre-trained models from HuggingFace","title":"Use pre-trained models from HuggingFace"},{"location":"advanced/PreTrainedModelsHF/#use-pre-trained-models-from-huggingface","text":"","title":"Use pre-trained models from HuggingFace"},{"location":"contribute/how_to_contribute/","text":"How to contribute to Melusine","title":"How to contribute to Melusine"},{"location":"contribute/how_to_contribute/#how-to-contribute-to-melusine","text":"","title":"How to contribute to Melusine"},{"location":"contribute/maif/","text":"MAIF","title":"MAIF"},{"location":"contribute/maif/#maif","text":"","title":"MAIF"},{"location":"history/history/","text":"Project history Melusine originated at MAIF in 2019. MAIF is a mutual insurance company funded in 1934 and based in Niort (France). MAIF is a \"Soci\u00e9t\u00e9 \u00e0 mission\" meaning that its activities are Motivation MAIF receives a large number of emails everyday and needs solutions to process them efficiently while maximizing customer satisfaction. Typical applications of automated email processing include: Email routing: Making sure that emails reach the most suited service to be processed Email prioritization: Treating the most urgent emails first Email summarization: Making it easy and fast for MAIF employees to grasp the email intention Open sourcing bla bla Refactoring From 2018 to 2023","title":"Project history"},{"location":"history/history/#project-history","text":"Melusine originated at MAIF in 2019. MAIF is a mutual insurance company funded in 1934 and based in Niort (France). MAIF is a \"Soci\u00e9t\u00e9 \u00e0 mission\" meaning that its activities are","title":"Project history"},{"location":"history/history/#motivation","text":"MAIF receives a large number of emails everyday and needs solutions to process them efficiently while maximizing customer satisfaction. Typical applications of automated email processing include: Email routing: Making sure that emails reach the most suited service to be processed Email prioritization: Treating the most urgent emails first Email summarization: Making it easy and fast for MAIF employees to grasp the email intention","title":"Motivation"},{"location":"history/history/#open-sourcing","text":"bla bla","title":"Open sourcing"},{"location":"history/history/#refactoring","text":"From 2018 to 2023","title":"Refactoring"},{"location":"philosophy/philosophy/","text":"Code philosophy What is a code philosophy and why do I need it ? Design patterns","title":"Code philosophy"},{"location":"philosophy/philosophy/#code-philosophy","text":"","title":"Code philosophy"},{"location":"philosophy/philosophy/#what-is-a-code-philosophy-and-why-do-i-need-it","text":"","title":"What is a code philosophy and why do I need it ?"},{"location":"philosophy/philosophy/#design-patterns","text":"","title":"Design patterns"},{"location":"tutorials/00_GettingStarted/","text":"Getting started with Melusine Let's run emergency detection with melusine : Load a fake email dataset Load a demonstration pipeline Run the pipeline Apply email cleaning transformations Apply emergency detection Input data Email datasets typically contain information about: Email sender Email recipients Email subject/header Email body Attachments data The present tutorial only makes use of the body and header data. body header 0 This is an \u00ebm\u00e8rg\u00e9n\u00e7y Help 1 How is life ? Hey ! 2 Urgent update about Mr. Annoying Latest news 3 Please call me now URGENT Code A typical code for a melusine-based application looks like this : from melusine.data import load_email_data from melusine.pipeline import MelusinePipeline # Load an email dataset df = load_email_data () # Load a pipeline pipeline = MelusinePipeline . from_config ( \"demo_pipeline\" ) # (1)! # Run the pipeline df = pipeline . transform ( df ) This tutorial uses one of the default pipeline configuration demo_pipeline . Melusine users will typically define their own pipeline configuration. See more in the Configurations tutorial Output data The pipeline created extra columns in the dataset. Some columns are temporary variables required by detectors (ex: normalized_body ) and some are detection results with direct business value (ex: emergency_result ). body header normalized_body emergency_result 0 This is an \u00ebm\u00e8rg\u00e9n\u00e7y Help This is an emergency True 1 How is life ? Hey ! How is life ? False 2 Urgent update about Mr. Annoying Latest news Urgent update about Mr. Annoying False 3 Please call me now URGENT Please call me now True Pipeline steps Illustration of the pipeline used in the present tutorial : --- title: Demonstration pipeline --- flowchart LR Input[[Email]] --> A(Cleaner) A(Cleaner) --> C(Normalizer) C --> F(Emergency\\nDetector) F --> Output[[Qualified Email]] Cleaner : Cleaning transformations such as uniformization of line breaks ( \\r\\n -> \\n ) Normalizer : Text normalisation to delete/replace non utf8 characters ( \u00e9\u00f6\u00e0 -> eoa ) EmergencyDetector : Detection of urgent emails Info This demonstration pipeline is kept minimal but typical pipelines include more complex preprocessing and a variety of detectors. For example, pipelines may contain: Email Segmentation : Split email conversation into unitary messages ContentTagging : Associate tags (SIGNATURE, FOOTER, BODY) to parts of messages Appointment detection : For exemple, detect \"construction work will take place on 01/01/2024\" as an appointment email. More on preprocessing in the MelusineTransformers tutorial More on detectors in the MelusineDetector tutorial Debug mode End users typically want to know what lead melusine to a specific detection result. The debug mode generates additional explainability info. from melusine.data import load_email_data from melusine.pipeline import MelusinePipeline # Load an email dataset df = load_email_data () # Activate debug mode df . debug = True # Load the default pipeline pipeline = MelusinePipeline . from_config ( \"demo_pipeline\" ) # Run the pipeline df = pipeline . transform ( df ) A new column debug_emergency is created. ... emergency_result debug_emergency 0 ... True [details_below] 1 ... False [details_below] 2 ... False [details_below] 3 ... True [details_below] Inspecting the debug data gives a lot of info: text : Effective text considered for detection. EmergencyRegex : melusine used an EmergencyRegex object to run detection. match_result : The EmergencyRegex did not match the text positive_match_data : The EmergencyRegex matched positively the text pattern \"Urgent\" (Required condition) negative_match_data : The EmergencyRegex matched negatively the text pattern \"Mr. Annoying\" (Forbidden condition) BLACKLIST : Detection groups can be defined to easily link a matching pattern to the corresponding regex. DEFAULT is used if no detection group is specified. # print(df.iloc[2][\"debug_emergency\"]) { 'text' : 'Latest news \\n Urgent update about Mr. Annoying' }, 'EmergencyRegex' : { 'match_result' : False , 'negative_match_data' : { 'BLACKLIST' : [ { 'match_text' : 'Mr. Annoying' , 'start' : 32 , 'stop' : 44 } ]}, 'neutral_match_data' : {}, 'positive_match_data' : { 'DEFAULT' : [ { 'match_text' : 'Urgent' , 'start' : 12 , 'stop' : 18 } ] } }","title":"Getting started with Melusine"},{"location":"tutorials/00_GettingStarted/#getting-started-with-melusine","text":"Let's run emergency detection with melusine : Load a fake email dataset Load a demonstration pipeline Run the pipeline Apply email cleaning transformations Apply emergency detection","title":"Getting started with Melusine"},{"location":"tutorials/00_GettingStarted/#input-data","text":"Email datasets typically contain information about: Email sender Email recipients Email subject/header Email body Attachments data The present tutorial only makes use of the body and header data. body header 0 This is an \u00ebm\u00e8rg\u00e9n\u00e7y Help 1 How is life ? Hey ! 2 Urgent update about Mr. Annoying Latest news 3 Please call me now URGENT","title":"Input data"},{"location":"tutorials/00_GettingStarted/#code","text":"A typical code for a melusine-based application looks like this : from melusine.data import load_email_data from melusine.pipeline import MelusinePipeline # Load an email dataset df = load_email_data () # Load a pipeline pipeline = MelusinePipeline . from_config ( \"demo_pipeline\" ) # (1)! # Run the pipeline df = pipeline . transform ( df ) This tutorial uses one of the default pipeline configuration demo_pipeline . Melusine users will typically define their own pipeline configuration. See more in the Configurations tutorial","title":"Code"},{"location":"tutorials/00_GettingStarted/#output-data","text":"The pipeline created extra columns in the dataset. Some columns are temporary variables required by detectors (ex: normalized_body ) and some are detection results with direct business value (ex: emergency_result ). body header normalized_body emergency_result 0 This is an \u00ebm\u00e8rg\u00e9n\u00e7y Help This is an emergency True 1 How is life ? Hey ! How is life ? False 2 Urgent update about Mr. Annoying Latest news Urgent update about Mr. Annoying False 3 Please call me now URGENT Please call me now True","title":"Output data"},{"location":"tutorials/00_GettingStarted/#pipeline-steps","text":"Illustration of the pipeline used in the present tutorial : --- title: Demonstration pipeline --- flowchart LR Input[[Email]] --> A(Cleaner) A(Cleaner) --> C(Normalizer) C --> F(Emergency\\nDetector) F --> Output[[Qualified Email]] Cleaner : Cleaning transformations such as uniformization of line breaks ( \\r\\n -> \\n ) Normalizer : Text normalisation to delete/replace non utf8 characters ( \u00e9\u00f6\u00e0 -> eoa ) EmergencyDetector : Detection of urgent emails Info This demonstration pipeline is kept minimal but typical pipelines include more complex preprocessing and a variety of detectors. For example, pipelines may contain: Email Segmentation : Split email conversation into unitary messages ContentTagging : Associate tags (SIGNATURE, FOOTER, BODY) to parts of messages Appointment detection : For exemple, detect \"construction work will take place on 01/01/2024\" as an appointment email. More on preprocessing in the MelusineTransformers tutorial More on detectors in the MelusineDetector tutorial","title":"Pipeline steps"},{"location":"tutorials/00_GettingStarted/#debug-mode","text":"End users typically want to know what lead melusine to a specific detection result. The debug mode generates additional explainability info. from melusine.data import load_email_data from melusine.pipeline import MelusinePipeline # Load an email dataset df = load_email_data () # Activate debug mode df . debug = True # Load the default pipeline pipeline = MelusinePipeline . from_config ( \"demo_pipeline\" ) # Run the pipeline df = pipeline . transform ( df ) A new column debug_emergency is created. ... emergency_result debug_emergency 0 ... True [details_below] 1 ... False [details_below] 2 ... False [details_below] 3 ... True [details_below] Inspecting the debug data gives a lot of info: text : Effective text considered for detection. EmergencyRegex : melusine used an EmergencyRegex object to run detection. match_result : The EmergencyRegex did not match the text positive_match_data : The EmergencyRegex matched positively the text pattern \"Urgent\" (Required condition) negative_match_data : The EmergencyRegex matched negatively the text pattern \"Mr. Annoying\" (Forbidden condition) BLACKLIST : Detection groups can be defined to easily link a matching pattern to the corresponding regex. DEFAULT is used if no detection group is specified. # print(df.iloc[2][\"debug_emergency\"]) { 'text' : 'Latest news \\n Urgent update about Mr. Annoying' }, 'EmergencyRegex' : { 'match_result' : False , 'negative_match_data' : { 'BLACKLIST' : [ { 'match_text' : 'Mr. Annoying' , 'start' : 32 , 'stop' : 44 } ]}, 'neutral_match_data' : {}, 'positive_match_data' : { 'DEFAULT' : [ { 'match_text' : 'Urgent' , 'start' : 12 , 'stop' : 18 } ] } }","title":"Debug mode"},{"location":"tutorials/01_MelusinePipeline/","text":"MelusinePipeline The MelusinePipeline class is at the core of melusine. It inherits from the sklearn.Pipeline class and adds extra functionalities such as : Instantiation from configurations Input/output coherence check Debug mode Code","title":"MelusinePipeline"},{"location":"tutorials/01_MelusinePipeline/#melusinepipeline","text":"The MelusinePipeline class is at the core of melusine. It inherits from the sklearn.Pipeline class and adds extra functionalities such as : Instantiation from configurations Input/output coherence check Debug mode","title":"MelusinePipeline"},{"location":"tutorials/01_MelusinePipeline/#code","text":"","title":"Code"},{"location":"tutorials/02_MelusineTransformers/","text":"MelusineTransformers","title":"MelusineTransformers"},{"location":"tutorials/02_MelusineTransformers/#melusinetransformers","text":"","title":"MelusineTransformers"},{"location":"tutorials/03_MelusineRegex/","text":"MelusineRegex","title":"MelusineRegex"},{"location":"tutorials/03_MelusineRegex/#melusineregex","text":"","title":"MelusineRegex"},{"location":"tutorials/04_UsingModels/","text":"Using AI models","title":"Using AI models"},{"location":"tutorials/04_UsingModels/#using-ai-models","text":"","title":"Using AI models"},{"location":"tutorials/05a_MelusineDetectors/","text":"Melusine Detectors The MelusineDetector component aims at standardizing how detection is performed in a MelusinePipeline . Tip Project running over several years (such as email automation) may accumulate technical debt over time. Standardizing code practices can limit the technical debt and ease the onboarding of new developers. The MelusineDetector class splits detection into three steps: pre_detect : Select/combine the inputs needed for detection. Ex: Select the text parts tagged as BODY and combine them with the text in the email header. detect : Use regular expressions, ML models or heuristics to run detection on the input text. post_detect : Run detection rules such as thresholding or combine results from multiple models. The method transform is defined by the BaseClass MelusineDetector and will call the pre_detect/detect/post_detect methods in turn (Template pattern). # Instantiate Detector detector = MyDetector () # Run pre_detect, detect and post_detect on input data data_with_detection = detector . transform ( data ) Here is the full code of a MelusineDetector to detect emails related to viruses. The next sections break down the different parts of the code. class MyCustomDetector ( BaseMelusineDetector ): @property def transform_methods ( self ) -> List [ Callable ]: return [ self . prepare , self . run ] def prepare ( self , row , debug_mode = False ): return row def run ( self , row , debug_mode = False ): row [ self . output_columns [ 0 ]] = \"12345\" return row The detector is run on a simple dataframe: df = pd . DataFrame ( [ { \"input_col\" : \"test1\" }, { \"input_col\" : \"test2\" }, ] ) detector = MyCustomDetector ( input_columns = [ \"input_col\" ], output_columns = [ \"output_col\" ], name = \"custom\" ) df = detector . transform ( df ) The output is a dataframe with a new virus_result column. body header virus_result 0 This is a dangerous virus test True 1 test test False 2 test viruses are dangerous True 3 corona virus is annoying test False Tip Columns that are not declared in the output_columns are dropped automatically. Detector init In the init method, you should call the superclass init and provide: A name for the detector Inputs columns Output columns Tip If the init method of the super class is enough (parameters name , input_columns and output_columns ) you may skip the init method entirely when defining your MelusineDetector . Detector pre_detect The pre_detect method simply combines the header text and the body text (separated by a line break). def pre_detect ( self , df , debug_mode = False ): # Assemble the text columns into a single column df [ self . TMP_DETECTION_INPUT_COLUMN ] = df [ self . header_column ] + \" \\n \" + df [ self . body_column ] return df Detector detect The detect applies two regexes on the selected text: - A positive regex to catch mentions to viruses - A negative regex to avoid false positive detections def detect ( self , df , debug_mode = False ): text_column = df [ self . TMP_DETECTION_INPUT_COLUMN ] positive_regex = r \"(virus)\" negative_regex = r \"(corona[ _]virus)\" # Pandas str.extract method on columns df [ self . TMP_POSITIVE_REGEX_MATCH ] = text_column . str . extract ( positive_regex ) . apply ( pd . notna ) df [ self . TMP_NEGATIVE_REGEX_MATCH ] = text_column . str . extract ( negative_regex ) . apply ( pd . notna ) return df Detector post_detect The post_detect combines the regex detection result to determine the final result. def post_detect ( self , df , debug_mode = False ): # Boolean operation on pandas column df [ self . OUTPUT_RESULT_COLUMN ] = df [ self . TMP_POSITIVE_REGEX_MATCH ] & ~ df [ self . TMP_NEGATIVE_REGEX_MATCH ] return df Are MelusineDetectors mandatory for melusine? No. You can use any scikit-learn compatible component in your MelusinePipeline . However, we recommend using the MelusineDetector (and MelusineTransformer ) classes to benefit from: Code standardization Input columns validation Dataframe backend variabilization Today dict and pandas backend are supported but more backends may be added (e.g. polars) Debug mode Multiprocessing Check-out the next tutorial to discover advanced features of the MelusineDetector class.","title":"Melusine Detectors"},{"location":"tutorials/05a_MelusineDetectors/#melusine-detectors","text":"The MelusineDetector component aims at standardizing how detection is performed in a MelusinePipeline . Tip Project running over several years (such as email automation) may accumulate technical debt over time. Standardizing code practices can limit the technical debt and ease the onboarding of new developers. The MelusineDetector class splits detection into three steps: pre_detect : Select/combine the inputs needed for detection. Ex: Select the text parts tagged as BODY and combine them with the text in the email header. detect : Use regular expressions, ML models or heuristics to run detection on the input text. post_detect : Run detection rules such as thresholding or combine results from multiple models. The method transform is defined by the BaseClass MelusineDetector and will call the pre_detect/detect/post_detect methods in turn (Template pattern). # Instantiate Detector detector = MyDetector () # Run pre_detect, detect and post_detect on input data data_with_detection = detector . transform ( data ) Here is the full code of a MelusineDetector to detect emails related to viruses. The next sections break down the different parts of the code. class MyCustomDetector ( BaseMelusineDetector ): @property def transform_methods ( self ) -> List [ Callable ]: return [ self . prepare , self . run ] def prepare ( self , row , debug_mode = False ): return row def run ( self , row , debug_mode = False ): row [ self . output_columns [ 0 ]] = \"12345\" return row The detector is run on a simple dataframe: df = pd . DataFrame ( [ { \"input_col\" : \"test1\" }, { \"input_col\" : \"test2\" }, ] ) detector = MyCustomDetector ( input_columns = [ \"input_col\" ], output_columns = [ \"output_col\" ], name = \"custom\" ) df = detector . transform ( df ) The output is a dataframe with a new virus_result column. body header virus_result 0 This is a dangerous virus test True 1 test test False 2 test viruses are dangerous True 3 corona virus is annoying test False Tip Columns that are not declared in the output_columns are dropped automatically.","title":"Melusine Detectors"},{"location":"tutorials/05a_MelusineDetectors/#detector-init","text":"In the init method, you should call the superclass init and provide: A name for the detector Inputs columns Output columns Tip If the init method of the super class is enough (parameters name , input_columns and output_columns ) you may skip the init method entirely when defining your MelusineDetector .","title":"Detector init"},{"location":"tutorials/05a_MelusineDetectors/#detector-pre_detect","text":"The pre_detect method simply combines the header text and the body text (separated by a line break). def pre_detect ( self , df , debug_mode = False ): # Assemble the text columns into a single column df [ self . TMP_DETECTION_INPUT_COLUMN ] = df [ self . header_column ] + \" \\n \" + df [ self . body_column ] return df","title":"Detector pre_detect"},{"location":"tutorials/05a_MelusineDetectors/#detector-detect","text":"The detect applies two regexes on the selected text: - A positive regex to catch mentions to viruses - A negative regex to avoid false positive detections def detect ( self , df , debug_mode = False ): text_column = df [ self . TMP_DETECTION_INPUT_COLUMN ] positive_regex = r \"(virus)\" negative_regex = r \"(corona[ _]virus)\" # Pandas str.extract method on columns df [ self . TMP_POSITIVE_REGEX_MATCH ] = text_column . str . extract ( positive_regex ) . apply ( pd . notna ) df [ self . TMP_NEGATIVE_REGEX_MATCH ] = text_column . str . extract ( negative_regex ) . apply ( pd . notna ) return df","title":"Detector detect"},{"location":"tutorials/05a_MelusineDetectors/#detector-post_detect","text":"The post_detect combines the regex detection result to determine the final result. def post_detect ( self , df , debug_mode = False ): # Boolean operation on pandas column df [ self . OUTPUT_RESULT_COLUMN ] = df [ self . TMP_POSITIVE_REGEX_MATCH ] & ~ df [ self . TMP_NEGATIVE_REGEX_MATCH ] return df","title":"Detector post_detect"},{"location":"tutorials/05a_MelusineDetectors/#are-melusinedetectors-mandatory-for-melusine","text":"No. You can use any scikit-learn compatible component in your MelusinePipeline . However, we recommend using the MelusineDetector (and MelusineTransformer ) classes to benefit from: Code standardization Input columns validation Dataframe backend variabilization Today dict and pandas backend are supported but more backends may be added (e.g. polars) Debug mode Multiprocessing Check-out the next tutorial to discover advanced features of the MelusineDetector class.","title":"Are MelusineDetectors mandatory for melusine?"},{"location":"tutorials/05b_MelusineDetectorsAdvanced/","text":"Advanced Melusine Detectors This tutorial presents the advanced features of the MelusineDetector class: Debug mode Row wise methods vs DataFrame wise methods Custom transform methods Debug mode MelusineDetector are designed to be easily debugged. For that purpose, the pre-detect/detect/post-detect methods all have a debug_mode argument. The debug mode is activated by setting the debug attribute of a dataframe to True. import pandas as pd df = pd . DataFrame ({ \"bla\" : [ 1 , 2 , 3 ]}) df . debug = True Warning Debug mode activation is backend dependent. With a DictBackend, tou should use my_dict[\"debug\"] = True When debug mode is activated, a column named \"DETECTOR_NAME_debug\" containing an empty dictionary is automatically created. Populating this debug dict with debug info is then left to the user's responsibility. Exemple of a detector with debug data class MyVirusDetector ( MelusineDetector ): OUTPUT_RESULT_COLUMN = \"virus_result\" TMP_DETECTION_INPUT_COLUMN = \"detection_input\" TMP_POSITIVE_REGEX_MATCH = \"positive_regex_match\" TMP_NEGATIVE_REGEX_MATCH = \"negative_regex_match\" def __init__ ( self , body_column : str , header_column : str ): self . body_column = body_column self . header_column = header_column super () . __init__ ( input_columns = [ self . body_column , self . header_column ], output_columns = [ self . OUTPUT_RESULT_COLUMN ], name = \"virus\" , ) def pre_detect ( self , row , debug_mode = False ): effective_text = row [ self . header_column ] + \" \\n \" + row [ self . body_column ] row [ self . TMP_DETECTION_INPUT_COLUMN ] = effective_text if debug_mode : row [ self . debug_dict_col ] = { \"detection_input\" : row [ self . TMP_DETECTION_INPUT_COLUMN ]} return row def detect ( self , row , debug_mode = False ): text = row [ self . TMP_DETECTION_INPUT_COLUMN ] positive_regex = r \"virus\" negative_regex = r \"corona[ _]virus\" positive_match = re . search ( positive_regex , text ) negative_match = re . search ( negative_regex , text ) row [ self . TMP_POSITIVE_REGEX_MATCH ] = bool ( positive_match ) row [ self . TMP_NEGATIVE_REGEX_MATCH ] = bool ( negative_match ) if debug_mode : positive_match_text = ( positive_match . string [ positive_match . start () : positive_match . end ()] if positive_match else None ) negative_match_text = ( positive_match . string [ negative_match . start () : negative_match . end ()] if negative_match else None ) row [ self . debug_dict_col ] . update ( { \"positive_match_data\" : { \"result\" : bool ( positive_match ), \"match_text\" : positive_match_text }, \"negative_match_data\" : { \"result\" : bool ( negative_match ), \"match_text\" : negative_match_text }, } ) return row def post_detect ( self , row , debug_mode = False ): if row [ self . TMP_POSITIVE_REGEX_MATCH ] and not row [ self . TMP_NEGATIVE_REGEX_MATCH ]: row [ self . OUTPUT_RESULT_COLUMN ] = True else : row [ self . OUTPUT_RESULT_COLUMN ] = False return row In the end, an extra column is created containing debug data: virus_result debug_virus 0 True {'detection_input': '...', 'positive_match_data': {'result': True, 'match_text': 'virus'}, 'negative_match_data': {'result': False, 'match_text': None}} 1 False {'detection_input': '...', 'positive_match_data': {'result': False, 'match_text': None}, 'negative_match_data': {'result': False, 'match_text': None}} 2 True {'detection_input': '...', 'positive_match_data': {'result': True, 'match_text': 'virus'}, 'negative_match_data': {'result': False, 'match_text': None}} 3 False {'detection_input': '...', 'positive_match_data': {'result': True, 'match_text': 'virus'}, 'negative_match_data': {'result': True, 'match_text': 'corona virus'}} Row methods vs dataframe methods There are two ways to use the pre-detect/detect/post-detect methods: Row wise: The method works on a single row of a DataFrame. In that case, a map-like method is used to apply it on an entire dataframe (typically pandas.DataFrame.apply is used with the PandasBackend) Dataframe wise: The method works directly on the entire DataFrame. Tip Using row wise methods make your code backend independent. You may switch from a PandasBackend to a DictBackend at any time. The PandasBackend also supports multiprocessing for row wise methods. To use row wise methods, you just need to name the first parameter of \"row\". Otherwise, dataframe wise transformations are used. Exemple of a Detector with dataframe wise method (works with a PandasBackend only). class MyVirusDetector ( MelusineDetector ): \"\"\" Detect if the text expresses dissatisfaction. \"\"\" # Dataframe column names OUTPUT_RESULT_COLUMN = \"virus_result\" TMP_DETECTION_INPUT_COLUMN = \"detection_input\" TMP_POSITIVE_REGEX_MATCH = \"positive_regex_match\" TMP_NEGATIVE_REGEX_MATCH = \"negative_regex_match\" def __init__ ( self , body_column : str , header_column : str ): self . body_column = body_column self . header_column = header_column super () . __init__ ( input_columns = [ self . body_column , self . header_column ], output_columns = [ self . OUTPUT_RESULT_COLUMN ], name = \"virus\" , ) def pre_detect ( self , df , debug_mode = False ): # Assemble the text columns into a single column df [ self . TMP_DETECTION_INPUT_COLUMN ] = df [ self . header_column ] + \" \\n \" + df [ self . body_column ] return df def detect ( self , df , debug_mode = False ): text_column = df [ self . TMP_DETECTION_INPUT_COLUMN ] positive_regex = r \"(virus)\" negative_regex = r \"(corona[ _]virus)\" # Pandas str.extract method on columns df [ self . TMP_POSITIVE_REGEX_MATCH ] = text_column . str . extract ( positive_regex ) . apply ( pd . notna ) df [ self . TMP_NEGATIVE_REGEX_MATCH ] = text_column . str . extract ( negative_regex ) . apply ( pd . notna ) return df def post_detect ( self , df , debug_mode = False ): # Boolean operation on pandas column df [ self . OUTPUT_RESULT_COLUMN ] = df [ self . TMP_POSITIVE_REGEX_MATCH ] & ~ df [ self . TMP_NEGATIVE_REGEX_MATCH ] return df Custom transform methods If you are not happy with the pre_detect / detect / post_detect transform methods, you: Use custom template methods Use regular pipeline steps (not inheriting from the MelusineDetector class) In this exemple, the prepare / run custom transform methods are used instead of the default pre_detect / detect / post_detect . class MyCustomDetector ( BaseMelusineDetector ): @property def transform_methods ( self ) -> List [ Callable ]: return [ self . prepare , self . run ] def prepare ( self , row , debug_mode = False ): return row def run ( self , row , debug_mode = False ): row [ self . output_columns [ 0 ]] = \"12345\" return row To configure custom transform methods you need to: inherit from the melusine.base.BaseMelusineDetector class define the transform_methods property The transform method will now call prepare and run . df = pd . DataFrame ( [ { \"input_col\" : \"test1\" }, { \"input_col\" : \"test2\" }, ] ) detector = MyCustomDetector ( input_columns = [ \"input_col\" ], output_columns = [ \"output_col\" ], name = \"custom\" ) df = detector . transform ( df ) We can check that the run method was indeed called. input_col output_col 0 test1 12345 1 test2 12345","title":"Advanced Melusine Detectors"},{"location":"tutorials/05b_MelusineDetectorsAdvanced/#advanced-melusine-detectors","text":"This tutorial presents the advanced features of the MelusineDetector class: Debug mode Row wise methods vs DataFrame wise methods Custom transform methods","title":"Advanced Melusine Detectors"},{"location":"tutorials/05b_MelusineDetectorsAdvanced/#debug-mode","text":"MelusineDetector are designed to be easily debugged. For that purpose, the pre-detect/detect/post-detect methods all have a debug_mode argument. The debug mode is activated by setting the debug attribute of a dataframe to True. import pandas as pd df = pd . DataFrame ({ \"bla\" : [ 1 , 2 , 3 ]}) df . debug = True Warning Debug mode activation is backend dependent. With a DictBackend, tou should use my_dict[\"debug\"] = True When debug mode is activated, a column named \"DETECTOR_NAME_debug\" containing an empty dictionary is automatically created. Populating this debug dict with debug info is then left to the user's responsibility. Exemple of a detector with debug data class MyVirusDetector ( MelusineDetector ): OUTPUT_RESULT_COLUMN = \"virus_result\" TMP_DETECTION_INPUT_COLUMN = \"detection_input\" TMP_POSITIVE_REGEX_MATCH = \"positive_regex_match\" TMP_NEGATIVE_REGEX_MATCH = \"negative_regex_match\" def __init__ ( self , body_column : str , header_column : str ): self . body_column = body_column self . header_column = header_column super () . __init__ ( input_columns = [ self . body_column , self . header_column ], output_columns = [ self . OUTPUT_RESULT_COLUMN ], name = \"virus\" , ) def pre_detect ( self , row , debug_mode = False ): effective_text = row [ self . header_column ] + \" \\n \" + row [ self . body_column ] row [ self . TMP_DETECTION_INPUT_COLUMN ] = effective_text if debug_mode : row [ self . debug_dict_col ] = { \"detection_input\" : row [ self . TMP_DETECTION_INPUT_COLUMN ]} return row def detect ( self , row , debug_mode = False ): text = row [ self . TMP_DETECTION_INPUT_COLUMN ] positive_regex = r \"virus\" negative_regex = r \"corona[ _]virus\" positive_match = re . search ( positive_regex , text ) negative_match = re . search ( negative_regex , text ) row [ self . TMP_POSITIVE_REGEX_MATCH ] = bool ( positive_match ) row [ self . TMP_NEGATIVE_REGEX_MATCH ] = bool ( negative_match ) if debug_mode : positive_match_text = ( positive_match . string [ positive_match . start () : positive_match . end ()] if positive_match else None ) negative_match_text = ( positive_match . string [ negative_match . start () : negative_match . end ()] if negative_match else None ) row [ self . debug_dict_col ] . update ( { \"positive_match_data\" : { \"result\" : bool ( positive_match ), \"match_text\" : positive_match_text }, \"negative_match_data\" : { \"result\" : bool ( negative_match ), \"match_text\" : negative_match_text }, } ) return row def post_detect ( self , row , debug_mode = False ): if row [ self . TMP_POSITIVE_REGEX_MATCH ] and not row [ self . TMP_NEGATIVE_REGEX_MATCH ]: row [ self . OUTPUT_RESULT_COLUMN ] = True else : row [ self . OUTPUT_RESULT_COLUMN ] = False return row In the end, an extra column is created containing debug data: virus_result debug_virus 0 True {'detection_input': '...', 'positive_match_data': {'result': True, 'match_text': 'virus'}, 'negative_match_data': {'result': False, 'match_text': None}} 1 False {'detection_input': '...', 'positive_match_data': {'result': False, 'match_text': None}, 'negative_match_data': {'result': False, 'match_text': None}} 2 True {'detection_input': '...', 'positive_match_data': {'result': True, 'match_text': 'virus'}, 'negative_match_data': {'result': False, 'match_text': None}} 3 False {'detection_input': '...', 'positive_match_data': {'result': True, 'match_text': 'virus'}, 'negative_match_data': {'result': True, 'match_text': 'corona virus'}}","title":"Debug mode"},{"location":"tutorials/05b_MelusineDetectorsAdvanced/#row-methods-vs-dataframe-methods","text":"There are two ways to use the pre-detect/detect/post-detect methods: Row wise: The method works on a single row of a DataFrame. In that case, a map-like method is used to apply it on an entire dataframe (typically pandas.DataFrame.apply is used with the PandasBackend) Dataframe wise: The method works directly on the entire DataFrame. Tip Using row wise methods make your code backend independent. You may switch from a PandasBackend to a DictBackend at any time. The PandasBackend also supports multiprocessing for row wise methods. To use row wise methods, you just need to name the first parameter of \"row\". Otherwise, dataframe wise transformations are used. Exemple of a Detector with dataframe wise method (works with a PandasBackend only). class MyVirusDetector ( MelusineDetector ): \"\"\" Detect if the text expresses dissatisfaction. \"\"\" # Dataframe column names OUTPUT_RESULT_COLUMN = \"virus_result\" TMP_DETECTION_INPUT_COLUMN = \"detection_input\" TMP_POSITIVE_REGEX_MATCH = \"positive_regex_match\" TMP_NEGATIVE_REGEX_MATCH = \"negative_regex_match\" def __init__ ( self , body_column : str , header_column : str ): self . body_column = body_column self . header_column = header_column super () . __init__ ( input_columns = [ self . body_column , self . header_column ], output_columns = [ self . OUTPUT_RESULT_COLUMN ], name = \"virus\" , ) def pre_detect ( self , df , debug_mode = False ): # Assemble the text columns into a single column df [ self . TMP_DETECTION_INPUT_COLUMN ] = df [ self . header_column ] + \" \\n \" + df [ self . body_column ] return df def detect ( self , df , debug_mode = False ): text_column = df [ self . TMP_DETECTION_INPUT_COLUMN ] positive_regex = r \"(virus)\" negative_regex = r \"(corona[ _]virus)\" # Pandas str.extract method on columns df [ self . TMP_POSITIVE_REGEX_MATCH ] = text_column . str . extract ( positive_regex ) . apply ( pd . notna ) df [ self . TMP_NEGATIVE_REGEX_MATCH ] = text_column . str . extract ( negative_regex ) . apply ( pd . notna ) return df def post_detect ( self , df , debug_mode = False ): # Boolean operation on pandas column df [ self . OUTPUT_RESULT_COLUMN ] = df [ self . TMP_POSITIVE_REGEX_MATCH ] & ~ df [ self . TMP_NEGATIVE_REGEX_MATCH ] return df","title":"Row methods vs dataframe methods"},{"location":"tutorials/05b_MelusineDetectorsAdvanced/#custom-transform-methods","text":"If you are not happy with the pre_detect / detect / post_detect transform methods, you: Use custom template methods Use regular pipeline steps (not inheriting from the MelusineDetector class) In this exemple, the prepare / run custom transform methods are used instead of the default pre_detect / detect / post_detect . class MyCustomDetector ( BaseMelusineDetector ): @property def transform_methods ( self ) -> List [ Callable ]: return [ self . prepare , self . run ] def prepare ( self , row , debug_mode = False ): return row def run ( self , row , debug_mode = False ): row [ self . output_columns [ 0 ]] = \"12345\" return row To configure custom transform methods you need to: inherit from the melusine.base.BaseMelusineDetector class define the transform_methods property The transform method will now call prepare and run . df = pd . DataFrame ( [ { \"input_col\" : \"test1\" }, { \"input_col\" : \"test2\" }, ] ) detector = MyCustomDetector ( input_columns = [ \"input_col\" ], output_columns = [ \"output_col\" ], name = \"custom\" ) df = detector . transform ( df ) We can check that the run method was indeed called. input_col output_col 0 test1 12345 1 test2 12345","title":"Custom transform methods"},{"location":"tutorials/06_Configurations/","text":"Configurations Melusine components can be instantiated using parameters defined in configurations. The from_config method accepts a config_dict argument from melusine.processors import Normalizer normalizer_conf = { \"input_columns\" : [ \"text\" ], \"output_columns\" : [ \"normalized_text\" ], \"form\" : \"NFKD\" , \"lowercase\" : False , } normalizer = Normalizer . from_config ( config_dict = normalizer_conf ) or a config_key argument. from melusine.pipeline import MelusinePipeline pipeline = MelusinePipeline . from_config ( config_key = \"demo_pipeline\" ) When demo_pipeline is given as argument, parameters are read from the melusine.config object at key demo_pipeline . Access configurations The melusine configurations can be accessed with the config object. from melusine import config print ( config [ \"demo_pipeline\" ]) The configuration of the demo_pipeline can then be easily inspected. { 'steps' : [ { 'class_name' : 'Cleaner' , 'config_key' : 'body_cleaner' , 'module' : 'melusine.processors' }, { 'class_name' : 'Cleaner' , 'config_key' : 'header_cleaner' , 'module' : 'melusine.processors' }, { 'class_name' : 'Segmenter' , 'config_key' : 'segmenter' , 'module' : 'melusine.processors' }, { 'class_name' : 'ContentTagger' , 'config_key' : 'content_tagger' , 'module' : 'melusine.processors' }, { 'class_name' : 'TextExtractor' , 'config_key' : 'text_extractor' , 'module' : 'melusine.processors' }, { 'class_name' : 'Normalizer' , 'config_key' : 'demo_normalizer' , 'module' : 'melusine.processors' }, { 'class_name' : 'EmergencyDetector' , 'config_key' : 'emergency_detector' , 'module' : 'melusine.detectors' } ] } Modify configurations The simplest way to modify configurations is to create a new directory directly. from melusine import config # Get a dict of the existing conf new_conf = config . dict () # Add/Modify a config key new_conf [ \"my_conf_key\" ] = \"my_conf_value\" # Reset Melusine configurations config . reset ( new_conf ) To deliver code in a production environment, using configuration files should be preferred to modifying the configurations on the fly. Melusine lets you specify the path to a folder containing yaml files and loads them (the OmegaConf package is used behind the scene). from melusine import config # Specify the path to a conf folder conf_path = \"path/to/conf/folder\" # Reset Melusine configurations config . reset ( config_path = conf_path ) # >> Using config_path : path/to/conf/folder When the MELUSINE_CONFIG_DIR environment variable is set, Melusine loads directly the configurations files located at the path specified by the environment variable. import os from melusine import config # Specify the MELUSINE_CONFIG_DIR environment variable os . environ [ \"MELUSINE_CONFIG_DIR\" ] = \"path/to/conf/folder\" # Reset Melusine configurations config . reset () # >> Using config_path from env variable MELUSINE_CONFIG_DIR # >> Using config_path : path/to/conf/folder Tip If the MELUSINE_CONFIG_DIR is set before melusine is imported (e.g., before starting the program), you don't need to call config.reset() . Export configurations Creating your configuration folder from scratch would be cumbersome. It is advised to export the default configurations and then modify just the files you need. from melusine import config # Specify the path a folder (created if it doesn't exist) conf_path = \"path/to/conf/folder\" # Export default configurations to the folder files_created = config . export_default_config ( path = conf_path ) Tip The export_default_config returns a list of path to all the files created.","title":"Configurations"},{"location":"tutorials/06_Configurations/#configurations","text":"Melusine components can be instantiated using parameters defined in configurations. The from_config method accepts a config_dict argument from melusine.processors import Normalizer normalizer_conf = { \"input_columns\" : [ \"text\" ], \"output_columns\" : [ \"normalized_text\" ], \"form\" : \"NFKD\" , \"lowercase\" : False , } normalizer = Normalizer . from_config ( config_dict = normalizer_conf ) or a config_key argument. from melusine.pipeline import MelusinePipeline pipeline = MelusinePipeline . from_config ( config_key = \"demo_pipeline\" ) When demo_pipeline is given as argument, parameters are read from the melusine.config object at key demo_pipeline .","title":"Configurations"},{"location":"tutorials/06_Configurations/#access-configurations","text":"The melusine configurations can be accessed with the config object. from melusine import config print ( config [ \"demo_pipeline\" ]) The configuration of the demo_pipeline can then be easily inspected. { 'steps' : [ { 'class_name' : 'Cleaner' , 'config_key' : 'body_cleaner' , 'module' : 'melusine.processors' }, { 'class_name' : 'Cleaner' , 'config_key' : 'header_cleaner' , 'module' : 'melusine.processors' }, { 'class_name' : 'Segmenter' , 'config_key' : 'segmenter' , 'module' : 'melusine.processors' }, { 'class_name' : 'ContentTagger' , 'config_key' : 'content_tagger' , 'module' : 'melusine.processors' }, { 'class_name' : 'TextExtractor' , 'config_key' : 'text_extractor' , 'module' : 'melusine.processors' }, { 'class_name' : 'Normalizer' , 'config_key' : 'demo_normalizer' , 'module' : 'melusine.processors' }, { 'class_name' : 'EmergencyDetector' , 'config_key' : 'emergency_detector' , 'module' : 'melusine.detectors' } ] }","title":"Access configurations"},{"location":"tutorials/06_Configurations/#modify-configurations","text":"The simplest way to modify configurations is to create a new directory directly. from melusine import config # Get a dict of the existing conf new_conf = config . dict () # Add/Modify a config key new_conf [ \"my_conf_key\" ] = \"my_conf_value\" # Reset Melusine configurations config . reset ( new_conf ) To deliver code in a production environment, using configuration files should be preferred to modifying the configurations on the fly. Melusine lets you specify the path to a folder containing yaml files and loads them (the OmegaConf package is used behind the scene). from melusine import config # Specify the path to a conf folder conf_path = \"path/to/conf/folder\" # Reset Melusine configurations config . reset ( config_path = conf_path ) # >> Using config_path : path/to/conf/folder When the MELUSINE_CONFIG_DIR environment variable is set, Melusine loads directly the configurations files located at the path specified by the environment variable. import os from melusine import config # Specify the MELUSINE_CONFIG_DIR environment variable os . environ [ \"MELUSINE_CONFIG_DIR\" ] = \"path/to/conf/folder\" # Reset Melusine configurations config . reset () # >> Using config_path from env variable MELUSINE_CONFIG_DIR # >> Using config_path : path/to/conf/folder Tip If the MELUSINE_CONFIG_DIR is set before melusine is imported (e.g., before starting the program), you don't need to call config.reset() .","title":"Modify configurations"},{"location":"tutorials/06_Configurations/#export-configurations","text":"Creating your configuration folder from scratch would be cumbersome. It is advised to export the default configurations and then modify just the files you need. from melusine import config # Specify the path a folder (created if it doesn't exist) conf_path = \"path/to/conf/folder\" # Export default configurations to the folder files_created = config . export_default_config ( path = conf_path ) Tip The export_default_config returns a list of path to all the files created.","title":"Export configurations"},{"location":"tutorials/07_BasicClassification/","text":"Zero Shot Classification Machine Learning is commonly used to classify data into pre-defined categories. --- title: Email classification --- flowchart LR Input[[Email]] --> X(((Classifier))) X --> A(Car) X --> B(Boat) X --> C(Housing) X --> D(Health) Typically, to reach high classification performance, models need to be trained on context specific labeled data. Zero-shot classification is a type of classification that uses a pre-trained model and does not require further training on context specific data. Tutorial intro In this tutorial we want to detect insatisfaction in an email dataset. Let's create a basic dataset: import pandas as pd from transformers import pipeline from melusine.base import MelusineDetector def create_dataset (): df = pd . DataFrame ( [ { \"header\" : \"Dossier 123456\" , \"body\" : \"Merci beaucoup pour votre gentillesse et votre \u00e9coute !\" , }, { \"header\" : \"R\u00e9clamation (Dossier 987654)\" , \"body\" : ( \"Bonjour, je ne suis pas satisfait de cette situation, \" \"r\u00e9pondez-moi rapidement svp!\" ), }, ] ) return df header body 0 Dossier 123456 Merci beaucoup pour votre gentillesse et votre \u00e9coute ! 1 R\u00e9clamation (Dossier 987654) Bonjour, je ne suis pas satisfait de cette situation, r\u00e9pondez-moi rapidement svp! Classify with Zero-Shot-Classification The transformers library makes it really simple to use pre-trained models for zero shot classification. model_name_or_path = \"cmarkea/distilcamembert-base-nli\" sentences = [ \"Quelle belle journ\u00e9e aujourd'hui\" , \"La mar\u00e9e est haute\" , \"Ce film est une catastrophe, je suis en col\u00e8re\" , ] classifier = pipeline ( task = \"zero-shot-classification\" , model = model_name_or_path , tokenizer = model_name_or_path ) result = classifier ( sequences = sentences , candidate_labels = \", \" . join ([ \"positif\" , \"n\u00e9gatif\" ]), hypothesis_template = \"Ce texte est {} .\" ) The classifier returns a score for the \"positif\" and \"n\u00e9gatif\" label for each input text: [ { 'seque n ce' : \"Quelle belle journ\u00e9e aujourd'hui\" , 'labels' : [ 'posi t i f ' , ' n \u00e9ga t i f ' ], 'scores' : [ 0.95 , 0.05 ] }, { 'seque n ce' : 'La mar\u00e9e es t hau te ' , 'labels' : [ 'posi t i f ' , ' n \u00e9ga t i f ' ], 'scores' : [ 0.76 , 0.24 ] }, { 'seque n ce' : 'Ce f ilm es t u ne ca tastr ophe , je suis e n col\u00e8re' , 'labels' : [ ' n \u00e9ga t i f ' , 'posi t i f ' ], 'scores' : [ 0.97 , 0.03 ] } ] Implement a Dissatisfaction detector A full email processing pipeline could contain multiple models. Melusine uses the MelusineDetector template class to standardise how models are integrated into a pipeline. class DissatisfactionDetector ( MelusineDetector ): \"\"\" Detect if the text expresses dissatisfaction. \"\"\" # Dataframe column names OUTPUT_RESULT_COLUMN = \"dissatisfaction_result\" TMP_DETECTION_INPUT_COLUMN = \"detection_input\" TMP_DETECTION_OUTPUT_COLUMN = \"detection_output\" # Model inference parameters POSITIVE_LABEL = \"positif\" NEGATIVE_LABEL = \"n\u00e9gatif\" HYPOTHESIS_TEMPLATE = \"Ce texte est {} .\" def __init__ ( self , model_name_or_path : str , text_columns : List [ str ], threshold : float ): self . text_columns = text_columns self . threshold = threshold self . classifier = pipeline ( task = \"zero-shot-classification\" , model = model_name_or_path , tokenizer = model_name_or_path ) super () . __init__ ( input_columns = text_columns , output_columns = [ self . OUTPUT_RESULT_COLUMN ], name = \"dissatisfaction\" ) The pre_detect method assembles the text that we want to use for classification. def pre_detect ( self , row , debug_mode = False ): # Assemble the text columns into a single text effective_text = \"\" for col in self . text_columns : effective_text += \" \\n \" + row [ col ] row [ self . TMP_DETECTION_INPUT_COLUMN ] = effective_text # Store the effective detection text in the debug data if debug_mode : row [ self . debug_dict_col ] = { \"detection_input\" : row [ self . TMP_DETECTION_INPUT_COLUMN ]} return row The detect method runs the classification model on the text. def detect ( self , row , debug_mode = False ): # Run the classifier on the text pipeline_result = self . classifier ( sequences = row [ self . TMP_DETECTION_INPUT_COLUMN ], candidate_labels = \", \" . join ([ self . POSITIVE_LABEL , self . NEGATIVE_LABEL ]), hypothesis_template = self . HYPOTHESIS_TEMPLATE , ) # Format classification result result_dict = dict ( zip ( pipeline_result [ \"labels\" ], pipeline_result [ \"scores\" ])) row [ self . TMP_DETECTION_OUTPUT_COLUMN ] = result_dict # Store ML results in the debug data if debug_mode : row [ self . debug_dict_col ] . update ( result_dict ) return row The post_detect method applies a threshold on the prediction score to determine the detection result. def post_detect ( self , row , debug_mode = False ): # Compare classification score to the detection threshold if row [ self . TMP_DETECTION_OUTPUT_COLUMN ][ self . NEGATIVE_LABEL ] > self . threshold : row [ self . OUTPUT_RESULT_COLUMN ] = True else : row [ self . OUTPUT_RESULT_COLUMN ] = False return row On top of that, the detector takes care of building debug data to make the result explicable. Run detection Putting it all together, we run the detector on the input dataset. df = create_dataset () detector = DissatisfactionDetector ( model_name_or_path = \"cmarkea/distilcamembert-base-nli\" , text_columns = [ \"header\" , \"body\" ], threshold = 0.7 , ) df = detector . transform ( df ) As a result, we get a new column dissatisfaction_result with the detection result. We could have detection details by running the detector in debug mode. header body dissatisfaction_result 0 Dossier 123456 Merci beaucoup pour votre gentillesse et votre \u00e9coute ! False 1 R\u00e9clamation (Dossier 987654) Bonjour, je ne suis pas satisfait de cette situation, r\u00e9pondez-moi rapidement svp! True","title":"Zero Shot Classification"},{"location":"tutorials/07_BasicClassification/#zero-shot-classification","text":"Machine Learning is commonly used to classify data into pre-defined categories. --- title: Email classification --- flowchart LR Input[[Email]] --> X(((Classifier))) X --> A(Car) X --> B(Boat) X --> C(Housing) X --> D(Health) Typically, to reach high classification performance, models need to be trained on context specific labeled data. Zero-shot classification is a type of classification that uses a pre-trained model and does not require further training on context specific data.","title":"Zero Shot Classification"},{"location":"tutorials/07_BasicClassification/#tutorial-intro","text":"In this tutorial we want to detect insatisfaction in an email dataset. Let's create a basic dataset: import pandas as pd from transformers import pipeline from melusine.base import MelusineDetector def create_dataset (): df = pd . DataFrame ( [ { \"header\" : \"Dossier 123456\" , \"body\" : \"Merci beaucoup pour votre gentillesse et votre \u00e9coute !\" , }, { \"header\" : \"R\u00e9clamation (Dossier 987654)\" , \"body\" : ( \"Bonjour, je ne suis pas satisfait de cette situation, \" \"r\u00e9pondez-moi rapidement svp!\" ), }, ] ) return df header body 0 Dossier 123456 Merci beaucoup pour votre gentillesse et votre \u00e9coute ! 1 R\u00e9clamation (Dossier 987654) Bonjour, je ne suis pas satisfait de cette situation, r\u00e9pondez-moi rapidement svp!","title":"Tutorial intro"},{"location":"tutorials/07_BasicClassification/#classify-with-zero-shot-classification","text":"The transformers library makes it really simple to use pre-trained models for zero shot classification. model_name_or_path = \"cmarkea/distilcamembert-base-nli\" sentences = [ \"Quelle belle journ\u00e9e aujourd'hui\" , \"La mar\u00e9e est haute\" , \"Ce film est une catastrophe, je suis en col\u00e8re\" , ] classifier = pipeline ( task = \"zero-shot-classification\" , model = model_name_or_path , tokenizer = model_name_or_path ) result = classifier ( sequences = sentences , candidate_labels = \", \" . join ([ \"positif\" , \"n\u00e9gatif\" ]), hypothesis_template = \"Ce texte est {} .\" ) The classifier returns a score for the \"positif\" and \"n\u00e9gatif\" label for each input text: [ { 'seque n ce' : \"Quelle belle journ\u00e9e aujourd'hui\" , 'labels' : [ 'posi t i f ' , ' n \u00e9ga t i f ' ], 'scores' : [ 0.95 , 0.05 ] }, { 'seque n ce' : 'La mar\u00e9e es t hau te ' , 'labels' : [ 'posi t i f ' , ' n \u00e9ga t i f ' ], 'scores' : [ 0.76 , 0.24 ] }, { 'seque n ce' : 'Ce f ilm es t u ne ca tastr ophe , je suis e n col\u00e8re' , 'labels' : [ ' n \u00e9ga t i f ' , 'posi t i f ' ], 'scores' : [ 0.97 , 0.03 ] } ]","title":"Classify with Zero-Shot-Classification"},{"location":"tutorials/07_BasicClassification/#implement-a-dissatisfaction-detector","text":"A full email processing pipeline could contain multiple models. Melusine uses the MelusineDetector template class to standardise how models are integrated into a pipeline. class DissatisfactionDetector ( MelusineDetector ): \"\"\" Detect if the text expresses dissatisfaction. \"\"\" # Dataframe column names OUTPUT_RESULT_COLUMN = \"dissatisfaction_result\" TMP_DETECTION_INPUT_COLUMN = \"detection_input\" TMP_DETECTION_OUTPUT_COLUMN = \"detection_output\" # Model inference parameters POSITIVE_LABEL = \"positif\" NEGATIVE_LABEL = \"n\u00e9gatif\" HYPOTHESIS_TEMPLATE = \"Ce texte est {} .\" def __init__ ( self , model_name_or_path : str , text_columns : List [ str ], threshold : float ): self . text_columns = text_columns self . threshold = threshold self . classifier = pipeline ( task = \"zero-shot-classification\" , model = model_name_or_path , tokenizer = model_name_or_path ) super () . __init__ ( input_columns = text_columns , output_columns = [ self . OUTPUT_RESULT_COLUMN ], name = \"dissatisfaction\" ) The pre_detect method assembles the text that we want to use for classification. def pre_detect ( self , row , debug_mode = False ): # Assemble the text columns into a single text effective_text = \"\" for col in self . text_columns : effective_text += \" \\n \" + row [ col ] row [ self . TMP_DETECTION_INPUT_COLUMN ] = effective_text # Store the effective detection text in the debug data if debug_mode : row [ self . debug_dict_col ] = { \"detection_input\" : row [ self . TMP_DETECTION_INPUT_COLUMN ]} return row The detect method runs the classification model on the text. def detect ( self , row , debug_mode = False ): # Run the classifier on the text pipeline_result = self . classifier ( sequences = row [ self . TMP_DETECTION_INPUT_COLUMN ], candidate_labels = \", \" . join ([ self . POSITIVE_LABEL , self . NEGATIVE_LABEL ]), hypothesis_template = self . HYPOTHESIS_TEMPLATE , ) # Format classification result result_dict = dict ( zip ( pipeline_result [ \"labels\" ], pipeline_result [ \"scores\" ])) row [ self . TMP_DETECTION_OUTPUT_COLUMN ] = result_dict # Store ML results in the debug data if debug_mode : row [ self . debug_dict_col ] . update ( result_dict ) return row The post_detect method applies a threshold on the prediction score to determine the detection result. def post_detect ( self , row , debug_mode = False ): # Compare classification score to the detection threshold if row [ self . TMP_DETECTION_OUTPUT_COLUMN ][ self . NEGATIVE_LABEL ] > self . threshold : row [ self . OUTPUT_RESULT_COLUMN ] = True else : row [ self . OUTPUT_RESULT_COLUMN ] = False return row On top of that, the detector takes care of building debug data to make the result explicable.","title":"Implement a Dissatisfaction detector"},{"location":"tutorials/07_BasicClassification/#run-detection","text":"Putting it all together, we run the detector on the input dataset. df = create_dataset () detector = DissatisfactionDetector ( model_name_or_path = \"cmarkea/distilcamembert-base-nli\" , text_columns = [ \"header\" , \"body\" ], threshold = 0.7 , ) df = detector . transform ( df ) As a result, we get a new column dissatisfaction_result with the detection result. We could have detection details by running the detector in debug mode. header body dissatisfaction_result 0 Dossier 123456 Merci beaucoup pour votre gentillesse et votre \u00e9coute ! False 1 R\u00e9clamation (Dossier 987654) Bonjour, je ne suis pas satisfait de cette situation, r\u00e9pondez-moi rapidement svp! True","title":"Run detection"}]}