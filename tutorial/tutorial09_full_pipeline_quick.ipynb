{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full pipeline (quick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains the full pipeline in a detailed manner, including the preprocessing steps, the summerization steps and the classification ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset under the Pandas Dataframe format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Melusine operates Pandas Dataframes by applying functions to certain columns to produce new columns, the initial columns have to follow a strict naming.\n",
    "\n",
    "The basic requirement to use Melusine is to have an input e-mail DataFrame with the following columns :\n",
    "- body : Body of an email (single message or conversation historic)\n",
    "- header : Header of an email\n",
    "- date : Reception date of an email\n",
    "- from : Email address of the sender\n",
    "- to (optional): Email address of the recipient\n",
    "- attachment (optional) : List of filenames attached to the email\n",
    "- label (optional): Label of the email for a classification task (examples: Business, Spam, Finance or Family)\n",
    "\n",
    "Each row correspond to a unique email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.data.data_loader import load_email_data\n",
    "import ast\n",
    "\n",
    "df_emails = load_email_data()\n",
    "df_emails['attachment'] = df_emails['attachment'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['body', 'header', 'date', 'from', 'to', 'attachment', 'sexe', 'age',\n",
       "       'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emails.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body :\n",
      " \n",
      "  \n",
      "  \n",
      "  \n",
      " ----- Transféré par Conseiller le 25/05/2018 08:20 ----- \n",
      "  \n",
      " De :\tDupont <monsieurdupont@extensiona.com> \n",
      " A :\tconseiller@Societeimaginaire.fr \n",
      " Date :\t24/05/2018 19:37 \n",
      " Objet :\tImmatriculation voiture \n",
      "  \n",
      "  \n",
      "  \n",
      " Bonsoir madame, \n",
      "  \n",
      " Je vous informe que la nouvelle immatriculation est enfin \n",
      " faite. Je vous remercie bien pour votre patience. \n",
      " Je vous prie de trouver donc la carte grise ainsi que la \n",
      " nouvelle immatriculation. Je vous demanderai de faire les changements \n",
      " nécessaires concernant l’assurance. \n",
      " Je vous remercie encore pour tout. \n",
      " Cordialement, \n",
      " Monsieur Dupont (See attached file: pj.pdf)\n",
      "\n",
      "\n",
      "Header :\n",
      "Tr : Immatriculation voiture\n",
      "Date :\n",
      "vendredi 25 mai 2018 06 h 21 CEST\n",
      "From :\n",
      "conseiller1@societeimaginaire.fr\n",
      "To :\n",
      "demandes@societeimaginaire.fr\n",
      "Attachment :\n",
      "['pj.pdf']\n",
      "Label :\n",
      "vehicule\n"
     ]
    }
   ],
   "source": [
    "print('Body :')\n",
    "print(df_emails.body[1])\n",
    "print('\\n')\n",
    "print('Header :')\n",
    "print(df_emails.header[1])\n",
    "print('Date :')\n",
    "print(df_emails.date[1])\n",
    "print('From :')\n",
    "print(df_emails.loc[1,\"from\"])\n",
    "print('To :')\n",
    "print(df_emails.to[1])\n",
    "print('Attachment :')\n",
    "print(df_emails.attachment[1])\n",
    "print('Label :')\n",
    "print(df_emails.label[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline will :\n",
    "- Update the columns of the dataframe if an email is transfered.\n",
    "- Segment the different messages of an email and tag its parts (hello, body, greetings, footer..).\n",
    "- Extract the body of the last message of the email.\n",
    "- Clean the body of the last message of the email.\n",
    "- Apply the phraser on the cleaned body.\n",
    "- Tokenize the cleaned body (once the phraser has been applied).\n",
    "\n",
    "The pipeline will return new columns at each steps, the most importants being :\n",
    "- **clean_body :** the body (with hello, greetings, signature, footers..) of the last message of an email, after cleaning and application of the phraser. This column will be used to train the embeddings and the neural networks\n",
    "- **tokens :** clean_body after tokenization. This column will be used for the keywords extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from melusine.utils.multiprocessing import apply_by_multiprocessing\n",
    "from melusine.utils.transformer_scheduler import TransformerScheduler\n",
    "\n",
    "from melusine.prepare_email.manage_transfer_reply import check_mail_begin_by_transfer\n",
    "from melusine.prepare_email.manage_transfer_reply import update_info_for_transfer_mail\n",
    "from melusine.prepare_email.manage_transfer_reply import add_boolean_transfer\n",
    "from melusine.prepare_email.manage_transfer_reply import add_boolean_answer\n",
    "\n",
    "from melusine.prepare_email.build_historic import build_historic\n",
    "from melusine.prepare_email.mail_segmenting import structure_email\n",
    "from melusine.prepare_email.body_header_extraction import extract_last_body\n",
    "from melusine.prepare_email.cleaning import clean_body\n",
    "from melusine.prepare_email.cleaning import clean_header\n",
    "\n",
    "from melusine.nlp_tools.phraser import Phraser\n",
    "from melusine.nlp_tools.phraser import phraser_on_body\n",
    "from melusine.nlp_tools.phraser import phraser_on_header\n",
    "from melusine.nlp_tools.tokenizer import Tokenizer\n",
    "from melusine.nlp_tools.embedding import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer object to manage transfers and replies\n",
    "ManageTransferReply = TransformerScheduler(\n",
    "    functions_scheduler=[\n",
    "        (check_mail_begin_by_transfer, None, ['is_begin_by_transfer']),\n",
    "        (update_info_for_transfer_mail, None, None),\n",
    "        (add_boolean_answer, None, ['is_answer']),\n",
    "        (add_boolean_transfer, None, ['is_transfer'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Transformer object to segment the different messages in the email, parse their metadata and\n",
    "# tag the different part of the messages\n",
    "Segmenting = TransformerScheduler(\n",
    "    functions_scheduler=[\n",
    "        (build_historic, None, ['structured_historic']),\n",
    "        (structure_email, None, ['structured_body'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Transformer object to extract the body of the last message of the email and clean it as \n",
    "# well as the header\n",
    "LastBodyHeaderCleaning = TransformerScheduler(\n",
    "    functions_scheduler=[\n",
    "        (extract_last_body, None, ['last_body']),\n",
    "        (clean_body, None, ['clean_body']),\n",
    "        (clean_header, None, ['clean_header'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Transformer object to apply the phraser on the texts\n",
    "phraser = Phraser().load('./data/phraser.pickle')\n",
    "PhraserTransformer = TransformerScheduler(\n",
    "    functions_scheduler=[\n",
    "        (phraser_on_body, (phraser,), ['clean_body']),\n",
    "        (phraser_on_header, (phraser,), ['clean_header'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Tokenizer object\n",
    "tokenizer = Tokenizer(input_column=\"clean_body\")\n",
    "\n",
    "# Full preprocessing pipeline\n",
    "PreprocessingPipeline = Pipeline([\n",
    "    ('ManageTransferReply', ManageTransferReply),\n",
    "    ('Segmenting', Segmenting),\n",
    "    ('LastBodyHeaderCleaning', LastBodyHeaderCleaning),\n",
    "    ('PhraserTransformer', PhraserTransformer),\n",
    "    ('tokenizer', tokenizer)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emails = PreprocessingPipeline.fit_transform(df_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['body', 'header', 'date', 'from', 'to', 'attachment', 'sexe', 'age',\n",
       "       'label', 'is_begin_by_transfer', 'is_answer', 'is_transfer',\n",
       "       'structured_historic', 'structured_body', 'last_body', 'clean_body',\n",
       "       'clean_header', 'tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emails.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata have to be extracted before being dummified.\n",
    "\n",
    "This pipeline extractes the following metadata :\n",
    "- **extension :** from the \"from\" column.\n",
    "- **dayofweek :** from the date.\n",
    "- **hour :** from the date.\n",
    "- **min :** from the date.\n",
    "- **attachment_type :** from the attachment column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from melusine.prepare_email.metadata_engineering import MetaExtension\n",
    "from melusine.prepare_email.metadata_engineering import MetaDate\n",
    "from melusine.prepare_email.metadata_engineering import MetaAttachmentType\n",
    "from melusine.prepare_email.metadata_engineering import Dummifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to extract dummified metadata\n",
    "MetadataPipeline = Pipeline([\n",
    "    ('MetaExtension', MetaExtension()),\n",
    "    ('MetaDate', MetaDate()),\n",
    "    ('MetaAttachmentType',MetaAttachmentType()),\n",
    "    ('Dummifier', Dummifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = MetadataPipeline.fit_transform(df_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['extension__0', 'extension__1', 'extension__2', 'extension__3',\n",
       "       'extension__4', 'extension__5', 'extension__6', 'extension__7',\n",
       "       'extension__8', 'extension__9', 'dayofweek__0', 'dayofweek__1',\n",
       "       'dayofweek__2', 'dayofweek__3', 'dayofweek__4', 'dayofweek__5',\n",
       "       'dayofweek__6', 'hour__6', 'hour__8', 'hour__9', 'hour__10', 'hour__11',\n",
       "       'hour__12', 'hour__14', 'hour__15', 'hour__16', 'hour__17', 'hour__18',\n",
       "       'hour__19', 'hour__20', 'hour__22', 'min__2', 'min__3', 'min__4',\n",
       "       'min__6', 'min__7', 'min__9', 'min__10', 'min__11', 'min__12',\n",
       "       'min__15', 'min__16', 'min__19', 'min__22', 'min__28', 'min__30',\n",
       "       'min__32', 'min__33', 'min__36', 'min__37', 'min__38', 'min__39',\n",
       "       'min__40', 'min__44', 'min__45', 'min__49', 'min__52', 'min__54',\n",
       "       'min__56', 'min__58', 'attachment_type__0', 'attachment_type__1',\n",
       "       'attachment_type__2', 'attachment_type__3', 'attachment_type__4',\n",
       "       'attachment_type__5', 'attachment_type__6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extension__0</th>\n",
       "      <th>extension__1</th>\n",
       "      <th>extension__2</th>\n",
       "      <th>extension__3</th>\n",
       "      <th>extension__4</th>\n",
       "      <th>extension__5</th>\n",
       "      <th>extension__6</th>\n",
       "      <th>extension__7</th>\n",
       "      <th>extension__8</th>\n",
       "      <th>extension__9</th>\n",
       "      <th>...</th>\n",
       "      <th>min__54</th>\n",
       "      <th>min__56</th>\n",
       "      <th>min__58</th>\n",
       "      <th>attachment_type__0</th>\n",
       "      <th>attachment_type__1</th>\n",
       "      <th>attachment_type__2</th>\n",
       "      <th>attachment_type__3</th>\n",
       "      <th>attachment_type__4</th>\n",
       "      <th>attachment_type__5</th>\n",
       "      <th>attachment_type__6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   extension__0  extension__1  extension__2  extension__3  extension__4  \\\n",
       "0             0             1             0             0             0   \n",
       "1             0             1             0             0             0   \n",
       "2             0             1             0             0             0   \n",
       "3             0             0             0             0             1   \n",
       "4             0             1             0             0             0   \n",
       "\n",
       "   extension__5  extension__6  extension__7  extension__8  extension__9  ...  \\\n",
       "0             0             0             0             0             0  ...   \n",
       "1             0             0             0             0             0  ...   \n",
       "2             0             0             0             0             0  ...   \n",
       "3             0             0             0             0             0  ...   \n",
       "4             0             0             0             0             0  ...   \n",
       "\n",
       "   min__54  min__56  min__58  attachment_type__0  attachment_type__1  \\\n",
       "0        0        0        0                   0                   0   \n",
       "1        0        0        0                   0                   0   \n",
       "2        0        0        0                   0                   0   \n",
       "3        0        0        0                   0                   0   \n",
       "4        0        0        0                   0                   0   \n",
       "\n",
       "   attachment_type__2  attachment_type__3  attachment_type__4  \\\n",
       "0                   0                   0                   1   \n",
       "1                   0                   0                   0   \n",
       "2                   0                   0                   1   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   0   \n",
       "\n",
       "   attachment_type__5  attachment_type__6  \n",
       "0                   0                   0  \n",
       "1                   1                   0  \n",
       "2                   0                   0  \n",
       "3                   0                   1  \n",
       "4                   1                   0  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a tokens column exists, keywords can be extracted by using the KeywordsGenerator class :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.summarizer.keywords_generator import KeywordsGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_generator = KeywordsGenerator(n_max_keywords=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emails = keywords_generator.fit_transform(df_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "  \n",
      "  \n",
      "  \n",
      " Bonjour , \n",
      "  \n",
      " Veuillez recevoir le certificat de cession de mon véhicule afin que vous \n",
      " puissiez effectuer la résiliation de mon contrat. \n",
      " Je reviendrai vers vous afin d’assurer mon nouveau véhicule bientôt. \n",
      "  \n",
      " Bien à vous , \n",
      "  \n",
      " Mr DUPONT \n",
      "  \n",
      "  \n",
      "  \n",
      " (Embedded image moved to file: pic.jpg) \n",
      "  \n",
      "  \n",
      " Envoyé de mon iPad\n"
     ]
    }
   ],
   "source": [
    "print(df_emails.body[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'veuillez recevoir le certificat de cession de mon vehicule afin que vous puissiez effectuer la resiliation de mon contrat. je reviendrai vers vous afin dassurer mon nouveau vehicule bientot.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emails.clean_body[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['veuillez',\n",
       " 'recevoir',\n",
       " 'certificat',\n",
       " 'cession',\n",
       " 'vehicule',\n",
       " 'afin',\n",
       " 'puissiez',\n",
       " 'effectuer',\n",
       " 'resiliation',\n",
       " 'contrat',\n",
       " 'reviendrai',\n",
       " 'vers',\n",
       " 'afin',\n",
       " 'dassurer',\n",
       " 'nouveau',\n",
       " 'vehicule',\n",
       " 'bientot']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emails.tokens[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['veuillez', 'vehicule', 'afin', 'nouveau']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emails.keywords[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melusine offers a NeuralModel class to train, save, load and use for prediction any kind of neural networks based on Keras. \n",
    "Predefined architectures of RNN and CNN models using the cleaned body and the metadata of the emails are also offered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings have to be pretrained on the data set to be given as arguments of the neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.embedding import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = Embedding(input_column='clean_body',\n",
    "                                 workers=1,\n",
    "                                 min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10 03:58 - melusine.nlp_tools.embedding - INFO - Start training for embedding\n",
      "21/10 03:58 - melusine.nlp_tools.embedding - INFO - Done.\n"
     ]
    }
   ],
   "source": [
    "pretrained_embedding.train(df_emails) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X and y preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([df_emails['clean_body'],df_meta],axis=1)\n",
    "y = df_emails['label']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['clean_body', 'extension__0', 'extension__1', 'extension__2',\n",
       "       'extension__3', 'extension__4', 'extension__5', 'extension__6',\n",
       "       'extension__7', 'extension__8', 'extension__9', 'dayofweek__0',\n",
       "       'dayofweek__1', 'dayofweek__2', 'dayofweek__3', 'dayofweek__4',\n",
       "       'dayofweek__5', 'dayofweek__6', 'hour__6', 'hour__8', 'hour__9',\n",
       "       'hour__10', 'hour__11', 'hour__12', 'hour__14', 'hour__15', 'hour__16',\n",
       "       'hour__17', 'hour__18', 'hour__19', 'hour__20', 'hour__22', 'min__2',\n",
       "       'min__3', 'min__4', 'min__6', 'min__7', 'min__9', 'min__10', 'min__11',\n",
       "       'min__12', 'min__15', 'min__16', 'min__19', 'min__22', 'min__28',\n",
       "       'min__30', 'min__32', 'min__33', 'min__36', 'min__37', 'min__38',\n",
       "       'min__39', 'min__40', 'min__44', 'min__45', 'min__49', 'min__52',\n",
       "       'min__54', 'min__56', 'min__58', 'attachment_type__0',\n",
       "       'attachment_type__1', 'attachment_type__2', 'attachment_type__3',\n",
       "       'attachment_type__4', 'attachment_type__5', 'attachment_type__6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_body</th>\n",
       "      <th>extension__0</th>\n",
       "      <th>extension__1</th>\n",
       "      <th>extension__2</th>\n",
       "      <th>extension__3</th>\n",
       "      <th>extension__4</th>\n",
       "      <th>extension__5</th>\n",
       "      <th>extension__6</th>\n",
       "      <th>extension__7</th>\n",
       "      <th>extension__8</th>\n",
       "      <th>...</th>\n",
       "      <th>min__54</th>\n",
       "      <th>min__56</th>\n",
       "      <th>min__58</th>\n",
       "      <th>attachment_type__0</th>\n",
       "      <th>attachment_type__1</th>\n",
       "      <th>attachment_type__2</th>\n",
       "      <th>attachment_type__3</th>\n",
       "      <th>attachment_type__4</th>\n",
       "      <th>attachment_type__5</th>\n",
       "      <th>attachment_type__6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>je suis client chez vous pouvez vous m etablir...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>je vous informe que la nouvelle immatriculatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suite a notre conversation telephonique de  fl...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>je fais suite a votre mail. j'ai envoye mon bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>voici ci joint mon bulletin de salaire comme d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_body  extension__0  \\\n",
       "0  je suis client chez vous pouvez vous m etablir...             0   \n",
       "1  je vous informe que la nouvelle immatriculatio...             0   \n",
       "2  suite a notre conversation telephonique de  fl...             0   \n",
       "3  je fais suite a votre mail. j'ai envoye mon bu...             0   \n",
       "4  voici ci joint mon bulletin de salaire comme d...             0   \n",
       "\n",
       "   extension__1  extension__2  extension__3  extension__4  extension__5  \\\n",
       "0             1             0             0             0             0   \n",
       "1             1             0             0             0             0   \n",
       "2             1             0             0             0             0   \n",
       "3             0             0             0             1             0   \n",
       "4             1             0             0             0             0   \n",
       "\n",
       "   extension__6  extension__7  extension__8  ...  min__54  min__56  min__58  \\\n",
       "0             0             0             0  ...        0        0        0   \n",
       "1             0             0             0  ...        0        0        0   \n",
       "2             0             0             0  ...        0        0        0   \n",
       "3             0             0             0  ...        0        0        0   \n",
       "4             0             0             0  ...        0        0        0   \n",
       "\n",
       "   attachment_type__0  attachment_type__1  attachment_type__2  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   0                   0   \n",
       "2                   0                   0                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   0   \n",
       "\n",
       "   attachment_type__3  attachment_type__4  attachment_type__5  \\\n",
       "0                   0                   1                   0   \n",
       "1                   0                   0                   1   \n",
       "2                   0                   1                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   1   \n",
       "\n",
       "   attachment_type__6  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   1  \n",
       "4                   0  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 10,  3,  0,  0,  4,  7, 10,  1, 10,  2,  5, 10, 10,  4,  7,  7,\n",
       "       10,  0,  9,  4, 10,  4,  7, 10, 10,  6,  7,  3,  8, 10, 10, 10,  4,\n",
       "        7,  3,  5,  4,  4, 10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and predictions with a  CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.models.neural_architectures import cnn_model\n",
    "from melusine.models.train import NeuralModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = NeuralModel(architecture_function=cnn_model,\n",
    "                       pretrained_embedding=pretrained_embedding,\n",
    "                       text_input_column=\"clean_body\",\n",
    "                       meta_input_list=['extension', 'dayofweek','hour', 'min', 'attachment_type'],\n",
    "                       n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4467 - accuracy: 0.0500\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 943us/step - loss: 2.4065 - accuracy: 0.1000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 995us/step - loss: 2.3739 - accuracy: 0.0750\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3025 - accuracy: 0.1750\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 957us/step - loss: 2.2571 - accuracy: 0.2000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 896us/step - loss: 2.1082 - accuracy: 0.2750\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0714 - accuracy: 0.2500\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 872us/step - loss: 2.0532 - accuracy: 0.2750\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0956 - accuracy: 0.3000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 857us/step - loss: 1.9757 - accuracy: 0.4000\n"
     ]
    }
   ],
   "source": [
    "nn_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['habitation', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res = nn_model.predict(X)\n",
    "y_res = le.inverse_transform(y_res)\n",
    "y_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emails_prod_without_melusine",
   "language": "python",
   "name": "emails_prod_without_melusine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
