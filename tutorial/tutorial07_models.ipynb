{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models subpackage tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NeuralModel class is a generic class used to manage neural networks implemented with Keras. It offers methods to save, load, train and use for classification the neural networks.\n",
    "\n",
    "Melusine provides two built-in Keras model : cnn_model and rnn_model based on the models used in-house at Maif. However the user is free to implement neural networks tailored for its needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NeuralModel class can take as input either :\n",
    "- a text input : a cleaned text, usually the cleaned body or the concatenation of the cleaned body and the cleaned header.\n",
    "- a text input and a metadata input : the metadata input has to be dummified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "df_emails_preprocessed = pd.read_csv('./data/emails_preprocessed.csv', encoding='utf-8', sep=';')\n",
    "df_emails_preprocessed['clean_header'] = df_emails_preprocessed['clean_header'].astype(str)\n",
    "df_emails_preprocessed['clean_body'] = df_emails_preprocessed['clean_body'].astype(str)\n",
    "df_emails_preprocessed['attachment'] = df_emails_preprocessed['attachment'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['body', 'header', 'date', 'from', 'to', 'attachment', 'sexe', 'age',\n",
       "       'label', 'is_begin_by_transfer', 'is_answer', 'is_transfer',\n",
       "       'structured_historic', 'structured_body', 'last_body', 'clean_body',\n",
       "       'clean_header', 'tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emails_preprocessed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new clean_text column is the concatenation of the clean_header column and the clean_body column :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emails_preprocessed['clean_text'] = df_emails_preprocessed['clean_header'] + \" \" + df_emails_preprocessed['clean_body']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'devis habitation je suis client chez vous pouvez vous m etablir un devis pour mon fils qui souhaite louer lappartement suivant : 25 rue du rueimaginaire  flag_cp_ '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emails_preprocessed.clean_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the metadata used are :\n",
    "- the extension : gmail, outlook, wanadoo..\n",
    "- the day of the week at which the email has been sent\n",
    "- the hour at which the email has been sent\n",
    "- the minute at which the email has been sent\n",
    "- the attachment types : pdf, png .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv('./data/metadata.csv', encoding='utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['extension__0', 'extension__1', 'extension__2', 'extension__3',\n",
       "       'extension__4', 'extension__5', 'extension__6', 'extension__7',\n",
       "       'extension__8', 'dayofweek__0', 'dayofweek__1', 'dayofweek__3',\n",
       "       'dayofweek__4', 'hour__6', 'hour__8', 'hour__9', 'hour__10', 'hour__11',\n",
       "       'hour__12', 'hour__14', 'hour__15', 'hour__16', 'hour__17', 'hour__18',\n",
       "       'hour__20', 'hour__22', 'min__0', 'min__2', 'min__4', 'min__6',\n",
       "       'min__9', 'min__10', 'min__11', 'min__12', 'min__15', 'min__16',\n",
       "       'min__19', 'min__20', 'min__21', 'min__22', 'min__24', 'min__28',\n",
       "       'min__29', 'min__30', 'min__32', 'min__33', 'min__37', 'min__38',\n",
       "       'min__39', 'min__40', 'min__44', 'min__45', 'min__49', 'min__54',\n",
       "       'min__56', 'min__58', 'attachment_type__0', 'attachment_type__1',\n",
       "       'attachment_type__2', 'attachment_type__3', 'attachment_type__4',\n",
       "       'attachment_type__5', 'attachment_type__6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extension__0</th>\n",
       "      <th>extension__1</th>\n",
       "      <th>extension__2</th>\n",
       "      <th>extension__3</th>\n",
       "      <th>extension__4</th>\n",
       "      <th>extension__5</th>\n",
       "      <th>extension__6</th>\n",
       "      <th>extension__7</th>\n",
       "      <th>extension__8</th>\n",
       "      <th>dayofweek__0</th>\n",
       "      <th>...</th>\n",
       "      <th>min__54</th>\n",
       "      <th>min__56</th>\n",
       "      <th>min__58</th>\n",
       "      <th>attachment_type__0</th>\n",
       "      <th>attachment_type__1</th>\n",
       "      <th>attachment_type__2</th>\n",
       "      <th>attachment_type__3</th>\n",
       "      <th>attachment_type__4</th>\n",
       "      <th>attachment_type__5</th>\n",
       "      <th>attachment_type__6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   extension__0  extension__1  extension__2  extension__3  extension__4  \\\n",
       "0             0             0             0             0             0   \n",
       "1             0             0             0             0             0   \n",
       "2             1             0             0             0             0   \n",
       "3             0             0             0             1             0   \n",
       "4             1             0             0             0             0   \n",
       "\n",
       "   extension__5  extension__6  extension__7  extension__8  dayofweek__0  ...  \\\n",
       "0             0             0             0             1             0  ...   \n",
       "1             0             0             0             1             0  ...   \n",
       "2             0             0             0             0             0  ...   \n",
       "3             0             0             0             0             0  ...   \n",
       "4             0             0             0             0             0  ...   \n",
       "\n",
       "   min__54  min__56  min__58  attachment_type__0  attachment_type__1  \\\n",
       "0        0        0        0                   0                   0   \n",
       "1        0        0        0                   0                   0   \n",
       "2        0        0        0                   0                   0   \n",
       "3        0        0        0                   0                   0   \n",
       "4        0        0        0                   0                   0   \n",
       "\n",
       "   attachment_type__2  attachment_type__3  attachment_type__4  \\\n",
       "0                   0                   0                   1   \n",
       "1                   0                   0                   0   \n",
       "2                   0                   0                   1   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   0   \n",
       "\n",
       "   attachment_type__5  attachment_type__6  \n",
       "0                   0                   0  \n",
       "1                   1                   0  \n",
       "2                   0                   0  \n",
       "3                   0                   1  \n",
       "4                   1                   0  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is a Pandas dataframe with a clean_text column that will be used for the text input and columns containing the dummified metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([df_emails_preprocessed['clean_text'],df_meta],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y is a numpy array containing the encoded labels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = df_emails_preprocessed['label']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 10,  3,  0,  0,  4,  7, 10,  1, 10,  2,  5, 10, 10,  4,  7,  7,\n",
       "       10,  0,  9,  4, 10,  4,  7, 10, 10,  6,  7,  3,  8, 10, 10, 10,  4,\n",
       "        7,  3,  5,  4,  4, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NeuralModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.models.train import NeuralModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NeuralModel class is a generic class used to manage neural networks implemented with Keras. It offers methods to save, load, train and use for classification the neural networks.\n",
    "\n",
    "Its arguments are :\n",
    "- **architecture_function :** a function returning a Model instance from Keras.\n",
    "- **pretrained_embedding :** the pretrained embedding matrix as an numpy array.\n",
    "- **text_input_column :** the name of the column that will provide the text input, by default clean_text.\n",
    "- **meta_input_list :** the list of the names of the columns containing the metadata. If empty list or None the model is used without metadata. Default value, ['extension', 'dayofweek', 'hour', 'min'].\n",
    "- **vocab_size :** the size of vocabulary for neurol network model. Default value, 25000.\n",
    "- **seq_size :** the maximum size of input for neural model. Default value, 100.\n",
    "- **loss :** the loss function for training. Default value, 'categorical_crossentropy'.\n",
    "- **batch_size :** the size of batches for the training of the neural network model. Default value, 4096.\n",
    "- **n_epochs :** the number of epochs for the training of the neural network model. Default value, 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### architecture_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.models.neural_architectures import cnn_model, rnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**architecture_function** is a function returning a Model instance from Keras.\n",
    "Melusine provides two built-in neural networks : **cnn_model** and **rnn_model** based on the models used in-house at Maif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pretrained_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding have to be trained on the user's dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a3d3511ef9a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmelusine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/melusine/melusine/nlp_tools/embedding.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim-4.1.0-py3.7-linux-x86_64.egg/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim-4.1.0-py3.7-linux-x86_64.egg/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim-4.1.0-py3.7-linux-x86_64.egg/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim-4.1.0-py3.7-linux-x86_64.egg/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim-4.1.0-py3.7-linux-x86_64.egg/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim-4.1.0-py3.7-linux-x86_64.egg/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject"
     ]
    }
   ],
   "source": [
    "from melusine.nlp_tools.embedding import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = Embedding().load('./data/embedding.pickle') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralModel used with text and metadata input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This neural network model will use the **clean_text** column for the text input and the dummified **extension**, **dayofweek**, **hour** and **min** as metadata input :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = NeuralModel(architecture_function=cnn_model,\n",
    "                       pretrained_embedding=pretrained_embedding,\n",
    "                       text_input_column=\"clean_text\",\n",
    "                       meta_input_list=['extension','attachment_type', 'dayofweek', 'hour', 'min'],\n",
    "                       n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training, logs are saved in \"train\" situated in the data directory. Use tensorboard to follow training using \n",
    "- \"tensorboard --logdir data\" from your terminal  \n",
    "- directly from a notebook with \"%load_ext tensorboard\" and \"%tensorboard --logdir data\" magics command (see https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4256 - accuracy: 0.0750\n",
      "Epoch 2/10\n",
      "WARNING:tensorflow:From /home/78169t/.conda/envs/sentiment/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3756 - accuracy: 0.1000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3082 - accuracy: 0.2000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2003 - accuracy: 0.2250\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1307 - accuracy: 0.2750\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9719 - accuracy: 0.3250\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0063 - accuracy: 0.2500\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8990 - accuracy: 0.4000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9821 - accuracy: 0.2750\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9568 - accuracy: 0.2750\n"
     ]
    }
   ],
   "source": [
    "nn_model.fit(X,y,tensorboard_log_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../docs/_static/tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **save_nn_model** method saves :\n",
    "- the Keras model as a json file \n",
    "- the weights as a h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.save_nn_model(\"./data/nn_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the **save_nn_model** used the NeuralModel object can be saved as a pickle file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "_ = joblib.dump(nn_model,\"./data/nn_model.pickle\",compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NeuralModel saved as a pickle file has to be loaded first : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = joblib.load(\"./data/nn_model.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the Keras model and its weights can be loaded :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.load_nn_model(\"./data/nn_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res = nn_model.predict(X)\n",
    "y_res = le.inverse_transform(y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'habitation', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralModel used with only text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_emails_preprocessed[['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = NeuralModel(architecture_function=cnn_model,\n",
    "                       pretrained_embedding=pretrained_embedding,\n",
    "                       text_input_column=\"clean_text\",\n",
    "                       meta_input_list=None,\n",
    "                       n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/78169t/melusine/melusine/nlp_tools/tokenizer.py:136: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[\"tokens\"] = apply_func(X, self.tokenize)\n",
      "/home/78169t/melusine/melusine/nlp_tools/tokenizer.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[\"tokens\"] = apply_func(X, lambda x: x[\"tokens\"][0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.4109 - accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3690 - accuracy: 0.1750\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 955us/step - loss: 2.3057 - accuracy: 0.2500\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 937us/step - loss: 2.2123 - accuracy: 0.2750\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1520 - accuracy: 0.2750\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0161 - accuracy: 0.2500\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 854us/step - loss: 2.0743 - accuracy: 0.2750\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 971us/step - loss: 1.9278 - accuracy: 0.3250\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 975us/step - loss: 1.8520 - accuracy: 0.4000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 816us/step - loss: 1.8427 - accuracy: 0.4750\n"
     ]
    }
   ],
   "source": [
    "nn_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule',\n",
       "       'vehicule', 'vehicule', 'vehicule', 'vehicule', 'vehicule'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res = nn_model.predict(X)\n",
    "y_res = le.inverse_transform(y_res)\n",
    "y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "melusine-fork",
   "language": "python",
   "name": "melusine-fork"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
