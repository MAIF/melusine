{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melusine Tokenizer Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Melusine tokenizer is a key component of the Melusine package.  \n",
    "It takes care of the tokenization pipeline in a broad sens inclusing:  \n",
    "* Text normalization\n",
    "* Lowercasing\n",
    "* General text flagging\n",
    "* Tokenization (in the strict sens, split text into tokens)\n",
    "* Remove stopwords\n",
    "* (Efficient) Name flagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load emails data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine import load_email_data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emails_preprocessed = load_email_data(type=\"preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tokenizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenizer class splits a sentence-like string into a list of sub-strings (tokens). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of a Tokenizer object are :\n",
    "* tokenizer_regex: (str) Regex used to split the text into tokens\n",
    "* normalization: (str or None) Type of normalization to apply to the text  \n",
    "  (Possible values are None, ‘NFC’, ‘NFKC’, ‘NFD’, and ‘NFKD’)\n",
    "* lowercase: (bool) If True, lowercase the text\n",
    "* stopwords: (list) List of words to be removed\n",
    "* remove_stopwords: (bool) If True, stopwords removal is enabled\n",
    "* flag_dict: Dict: (dict) Flagging dict with regex as key and replace_text as value\n",
    "* collocations_dict: (dict) Dict with expressions to be grouped into one unit of sens\n",
    "* names: (list) List of names to be flagged\n",
    "name_flag: (str) Replace value for names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.340793Z",
     "start_time": "2019-09-03T13:16:56.335017Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.tokenizer import WordLevelTokenizer\n",
    "\n",
    "tokenizer = WordLevelTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the tokenize method to split text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.369279Z",
     "start_time": "2019-09-03T13:16:56.345282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bonjour',\n",
       " 'flag_name_',\n",
       " 'peux-tu',\n",
       " 'appeller',\n",
       " 'numero',\n",
       " 'suivant',\n",
       " 'flag_phone_']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Bonjour Mireille, peux-tu m'appeller au numéro suivant : 0612345678?\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.382607Z",
     "start_time": "2019-09-03T13:16:56.378519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [client, chez, pouvez, etablir, devis, fils, s...\n",
       "1    [informe, nouvelle, immatriculation, enfin, fa...\n",
       "2    [suite, a, conversation, telephonique, flag_da...\n",
       "3    [fais, suite, a, mail, envoye, bulletin, salai...\n",
       "4    [voici, ci, joint, bulletin, salaire, comme, d...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emails_preprocessed[\"tokens\"] = df_emails_preprocessed[\"clean_body\"].apply(tokenizer.tokenize)\n",
    "df_emails_preprocessed[\"tokens\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a Tokenizer\n",
    "The tokenizer is saved to a human readable json config file.  \n",
    "The file can be inspected and the tokenizer configurations can be modified easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.580240Z",
     "start_time": "2019-09-03T13:16:56.384169Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.save(\"./data/my_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a Tokenizer \n",
    "Load a tokenizer from a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.704821Z",
     "start_time": "2019-09-03T13:16:56.582517Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_reloaded = WordLevelTokenizer.load(\"./data/my_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Custom tokenization regex\n",
    "The default tokenizer does not split the text on the \"-\" character.  \n",
    "Let's customize the tokenizer for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default tokenization regex : \\w+(?:[\\?\\-\"_]\\w+)*\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['voulez-vous']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Default tokenization regex : {tokenizer_reloaded.tokenizer_regex}\")\n",
    "tokenizer_reloaded.tokenize(\"voulez-vous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['voulez', 'vous']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer_regex = r\"\\w+(?:[\\?\\\"_]\\w+)*\"\n",
    "custom_tokenizer = WordLevelTokenizer(tokenizer_regex=custom_tokenizer_regex, stopwords=None)\n",
    "custom_tokenizer.tokenize(\"voulez-vous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom flags\n",
    "Flagging consists in replacing expressions by a standard value.  \n",
    "Typical examples of flagging are:  \n",
    "* Replace phone numbers by a flag\n",
    "* Replace email addresses by a flag\n",
    "* etc\n",
    "\n",
    "With Melusine, you can easily define custom flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_flag_dict = {\n",
    "    \"chats?\": \"flag_animal\",\n",
    "    \"chiens?\": \"flag_animal\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flag_animal', 'apprecie', 'pas', 'flag_animal']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer = WordLevelTokenizer(flag_dict=custom_flag_dict)\n",
    "custom_tokenizer.tokenize(\"Mon chat n'apprécie pas nos chiens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom names\n",
    "Searching through a list with thousands of names may take long.  \n",
    "For performance optimization, Melusine uses the library Flashtext to flag names.  \n",
    "To use a custom name list, change the names parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flag_name_', 'rencontre', 'flag_name_', 'chateau']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer = WordLevelTokenizer(names=[\"daenerys\", \"tyrion\"])\n",
    "custom_tokenizer.tokenize(\"Daenerys rencontre Tyrion dans un chateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "NLP models performance may be improved by grouping together words forming a single unit of sens.  \n",
    "Ex:  \n",
    "* new york -> new_york\n",
    "* rendez vous -> rendez_vous\n",
    "\n",
    "To use custom collocations, change the collocations_dict parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rendez_vous', 'rendez_vous', 'rendez_vous', 'rendez_vous']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_collocations_dict = {\n",
    "    \"rdv\": \"rendez_vous\",\n",
    "    \"rendez[ -]+vous\": \"rendez_vous\",\n",
    "}\n",
    "custom_tokenizer = WordLevelTokenizer(collocations_dict=custom_collocations_dict, stopwords=None)\n",
    "custom_tokenizer.tokenize(\"rdv rendez vous rendez       vous rendez-vous\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melusine_perso38",
   "language": "python",
   "name": "melusine_perso38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
