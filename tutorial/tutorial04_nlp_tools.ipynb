{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from tempfile import TemporaryDirectory, TemporaryFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tools tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **nlp_tools** subpackage offers classic NLP tools implemented as classes that will be used to preprocess an already cleaned text :\n",
    "- a **Tokenizer class** : to split a sentence-like string into a list of sub-strings (tokens).\n",
    "- a **Phraser class** : to transform common multi-word expressions into single elements (*new york* becomes *new_york*)\n",
    "- an **Embedding class** : to represent of words in a lower dimensional vector space.\n",
    "- a **Stemmer class**: to reduce words to their word stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/78169t/melusine/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import melusine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_email_data() got an unexpected keyword argument 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48936/2697735229.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmelusine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_email_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_emails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_email_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"preprocessed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: load_email_data() got an unexpected keyword argument 'type'"
     ]
    }
   ],
   "source": [
    "from melusine.data.data_loader import load_email_data\n",
    "df_emails = load_email_data(type=\"preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tokenizer class\n",
    "The Tokenizer class splits a sentence-like string into a list of sub-strings (tokens).  \n",
    "\n",
    "The arguments of a Tokenizer object are just the input_columns and output_columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer (input_column='clean_body', output_column=\"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **fit_transform** method on a dataframe to create a new ***tokens* column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base text\n",
      "je vous informe que la nouvelle immatriculation est enfin faite. je vous prie de trouver donc la carte grise ainsi que la nouvelle immatriculation. je vous demanderai de faire les changements necessaires concernant lassurance.\n",
      "\n",
      "Tokenized text\n",
      "['informe', 'nouvelle', 'immatriculation', 'enfin', 'faite', 'prie', 'trouver', 'donc', 'carte', 'grise', 'ainsi', 'nouvelle', 'immatriculation', 'demanderai', 'faire', 'les', 'changements', 'necessaires', 'concernant', 'lassurance']\n"
     ]
    }
   ],
   "source": [
    "df_emails = tokenizer.fit_transform(df_emails)\n",
    "print(\"Base text\")\n",
    "print(df_emails.clean_body[1])\n",
    "print(\"\\nTokenized text\")\n",
    "print(df_emails.tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load / Save a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as tmpdir:\n",
    "    path = f\"{tmpdir}/tokenizer.pkl\"\n",
    "    _ = joblib.dump(tokenizer, path, compress=True)\n",
    "    tokenizer_reload = joblib.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['informe', 'nouvelle', 'immatriculation', 'enfin', 'faite', 'prie', 'trouver', 'donc', 'carte', 'grise', 'ainsi', 'nouvelle', 'immatriculation', 'demanderai', 'faire', 'les', 'changements', 'necessaires', 'concernant', 'lassurance']\n"
     ]
    }
   ],
   "source": [
    "df_emails = tokenizer_reload.fit_transform(df_emails)\n",
    "print(df_emails.tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Phraser class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Phraser class transforms common multi-word expressions into single elements: for example *new york* becomes *new_york*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of a Phraser object are:\n",
    "- **input_column :** the name of the column of the dataframe that will be used as input for the training of the Phraser.\n",
    "- **common_terms :** list of stopwords to be ignored. The default list is defined in the *conf.json* file.\n",
    "- **threshold :** threshold to select collocations.\n",
    "- **min_count :** minimum count of word to be selected as collocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.104064Z",
     "start_time": "2019-09-03T13:16:54.128719Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.phraser import Phraser\n",
    "\n",
    "phraser = Phraser(\n",
    "    input_column='tokens',\n",
    "    output_column='phrased_tokens',\n",
    "    threshold=5,\n",
    "    min_count=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input dataframe must contain a column with a clean text : **a sentence-like string with only lowcase letters and no accents**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.190813Z",
     "start_time": "2019-09-03T13:16:56.150379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fais', 'suite_a', 'mail', 'envoye', 'bulletin_salaire', 'courrier', 'semblerait', 'receptionne', 'trouverez', 'ci-joint', 'bulletin_salaire']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/78169t/.conda/envs/melusine/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "_ = phraser.fit(df_emails)\n",
    "df_emails = phraser.transform(df_emails)\n",
    "print(df_emails['phrased_tokens'].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result : You should see phrased tokens such as  \n",
    "\"bulletin\" + \"salaire\" = \"bulletin_salaire\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load/Save a phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.205955Z",
     "start_time": "2019-09-03T13:16:56.192989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fais', 'suite_a', 'mail', 'envoye', 'bulletin_salaire', 'courrier', 'semblerait', 'receptionne', 'trouverez', 'ci-joint', 'bulletin_salaire']\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as tmpdir:\n",
    "    path = f\"{tmpdir}/phraser.pkl\"\n",
    "    _ = joblib.dump(phraser, path, compress=True)\n",
    "    phraser_reload = joblib.load(path)\n",
    "    \n",
    "print(phraser_reload.transform(df_emails)['phrased_tokens'].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are abstract representations of words in a lower dimensional vector space. One of the advantages of word embeddings is thus to save computational cost. The Melusine Embedding class uses a **Word2Vec** model. The trained Embedding object will be used in the Models subpackage to train a Neural Network to classify emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of an Embedding object are :\n",
    "- **input_column :** the name of the column used as an input for the training.\n",
    "- **workers :** the number of cores used for computation. Default value, 40.\n",
    "- **seed :** seed for the embedding model,\n",
    "- **iter :** number of iterations for the training,\n",
    "- **size :** dimension of the embeddings\n",
    "- **window :** \n",
    "- **min_count :** minimum number of occurences for a word to be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.940504Z",
     "start_time": "2019-09-03T13:16:56.707282Z"
    }
   },
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.embedding import Embedding\n",
    "\n",
    "embedding = Embedding(\n",
    "    tokens_column='tokens',\n",
    "    size=300,\n",
    "    workers=4,\n",
    "    min_count=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T13:16:56.983525Z",
     "start_time": "2019-09-03T13:16:56.942514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contrat', 0.16994555294513702),\n",
       " ('imaginaire', 0.13840633630752563),\n",
       " ('trouver', 0.11760054528713226),\n",
       " ('bulletin', 0.11261097341775894),\n",
       " ('2', 0.09284727275371552),\n",
       " ('salaire', 0.09037808328866959),\n",
       " ('entretien', 0.060972344130277634),\n",
       " ('flag_time_', 0.05960045009851456),\n",
       " ('concernant', 0.059587351977825165),\n",
       " ('voici', 0.05908174812793732)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.train(df_emails)\n",
    "embedding.embedding.most_similar(\"vehicule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning : The embedding is trained on a very small dataset so the results here are irrelevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The stemmer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stemmer class reduces words (tokens) from a list to their word stem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of a Stemmer object are:\n",
    "- **input_column :** the name of the column of the dataframe that will be used as input for the reduction of the Stemmer.\n",
    "- **output_column :** the name of the column of the dataframe that will be used as output for the reduction of the Stemmer.\n",
    "- **language :** language used to stem words, default \"french\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from melusine.nlp_tools.stemmer import Stemmer\n",
    "\n",
    "stemmer = Stemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stemmer doesn't need any training. You can apply it directly on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fais', 'suit', 'a', 'mail', 'envoy', 'bulletin', 'salair', 'courri', 'sembl', 'reception', 'trouv', 'ci-joint', 'bulletin', 'salair']\n"
     ]
    }
   ],
   "source": [
    "df_emails = stemmer.transform(df_emails)\n",
    "print(df_emails['stemmed_tokens'].iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_melusine37",
   "language": "python",
   "name": "env_melusine37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
